@inproceedings{10.1145/2661829.2661935,
 author = {Yelong Shen et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {Proceedings of the 23rd {ACM} International Conference on Conference
on Information and Knowledge Management, {CIKM} 2014, Shanghai, China,
November 3-7, 2014},
 pages = {101--110},
 title = {A Latent Semantic Model with Convolutional-Pooling Structure for Information
Retrieval},
 year = {2014}
}

@inproceedings{10.1145/2766462.2767738,
 author = {Aliaksei Severyn et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {SIGIR},
 pages = {373--382},
 title = {Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks},
 year = {2015}
}

@inproceedings{10.1145/2911451.2911542,
 author = {Rui Yan et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {SIGIR},
 pages = {55--64},
 title = {Learning to Respond with Deep Neural Networks for Retrieval-Based
Human-Computer Conversation System},
 year = {2016}
}

@inproceedings{10.1145/3240508.3240605,
 author = {Lizi Liao et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ACM MM},
 crossref = {DBLP:conf/mm/2018},
 pages = {801--809},
 title = {Knowledge-aware Multimodal Dialogue Systems},
 year = {2018}
}

@inproceedings{10.1145/3331184.3331193,
 author = {Jiayi Zhang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {SIGIR},
 pages = {435--444},
 title = {EnsembleGAN: Adversarial Learning for Retrieval-Generation Ensemble
Model on Short-Text Conversation},
 year = {2019}
}

@inproceedings{10.1145/3357384.3357881,
 author = {Liu Yang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {Proceedings of the 28th {ACM} International Conference on Information
and Knowledge Management, {CIKM} 2019, Beijing, China, November 3-7,
2019},
 pages = {1341--1350},
 title = {A Hybrid Retrieval-Generation Neural Conversation Model},
 year = {2019}
}

@article{10.1145/3502198,
 abstract = {Spoken language understanding (SLU) has been addressed as a supervised learning problem, where a set of training data is available for each domain. However, annotating data for a new domain can be both financially costly and non-scalable. One existing approach solves the problem by conducting multi-domain learning where parameters are shared for joint training across domains, which is domain-agnostic and task-agnostic. In the article, we propose to improve the parameterization of this method by using domain-specific and task-specific model parameters for fine-grained knowledge representation and transfer. Experiments on five domains show that our model is more effective for multi-domain SLU and obtain the best results. In addition, we show its transferability when adapting to a new domain with little data, outperforming the prior best model by 12.4\%. Finally, we explore the strong pre-trained model in our framework and find that the contributions from our framework do not fully overlap with contextualized word representations (RoBERTa).},
 articleno = {77},
 author = {Qin Libo et al.},
 issue_date = {July 2022},
 journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
 keywords = {domain-specific and task-specific model, Multi-domain spoken language understanding, fine-grained knowledge representation and transfer},
 numpages = {17},
 title = {Multi-Domain Spoken Language Understanding Using Domain- and Task-Aware Parameterization},
 volume = {21},
 year = {2022}
}

@inproceedings{10.1145/3511808.3557359,
 abstract = {Endowing chatbots with a consistent personality plays a vital role for agents to deliver human-like interactions. However, existing personalized approaches commonly generate responses in light of static predefined personas depicted with textual description, which may severely restrict the interactivity of human and the chatbot, especially when the agent needs to answer the query excluded in the predefined personas, which is so-called out-of-predefined persona problem (named OOP for simplicity). To alleviate the problem, in this paper we propose a novel retrieval-to-prediction paradigm consisting of two subcomponents, namely, (1) Persona Retrieval Model (PRM), it retrieves a persona from a global collection based on a Natural Language Inference (NLI) model, the inferred persona is consistent with the predefined personas; and (2) Posterior-scored Transformer (PS-Transformer), it adopts a persona posterior distribution that further considers the actual personas used in the ground response, maximally mitigating the gap between training and inferring. Furthermore, we present a dataset called IT-ConvAI2 that first highlights the OOP problem in personalized dialogue. Extensive experiments on both IT-ConvAI2 and ConvAI2 demonstrate that our proposed model yields considerable improvements in both automatic metrics and human evaluations.},
 author = {Liu Yifan et al.},
 booktitle = {Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
 isbn = {9781450392365},
 keywords = {dialogue generation, personality consistency, persona expanding},
 location = {Atlanta, GA, USA},
 numpages = {10},
 pages = {1350–1359},
 series = {CIKM '22},
 title = {Improving Personality Consistency in Conversation by Persona Extending},
 year = {2022}
}

@article{10.1145/3623381,
 abstract = {Existing studies in conversational AI mostly treat task-oriented dialog (TOD) and question answering (QA) as separate tasks. Towards the goal of constructing a conversational agent that can complete user tasks and support information seeking, it is important to develop a system that can handle both TOD and QA with access to various external knowledge sources. In this work, we propose a new task, Open-Book TOD (OB-TOD), which combines TOD with QA and expands the external knowledge sources to include both explicit sources (e.g., the web) and implicit sources (e.g., pre-trained language models). We create a new dataset OB-MultiWOZ, where we enrich TOD sessions with QA-like information-seeking experience grounded on external knowledge. We propose a unified model OPERA (Open-book End-to-end Task-oriented Dialog) which can appropriately access explicit and implicit external knowledge to tackle the OB-TOD task. Experimental results show that OPERA outperforms closed-book baselines, highlighting the value of both types of knowledge.},
 author = {Li Miaoran et al.},
 journal = {ACM Trans. Web},
 keywords = {Web search, Language models, Task-oriented dialog systems},
 note = {Just Accepted},
 title = {OPERA: Harmonizing Task-Oriented Dialogs and Information Seeking Experience},
 year = {2023}
}

@inproceedings{10.1609/aaai.v33i01.33017281,
 author = {Yu Wu et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {7281--7288},
 title = {Response Generation by Context-Aware Prototype Editing},
 year = {2019}
}

@inproceedings{10.5555/2969033.2969055,
 author = {Baotian Hu et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 pages = {2042--2050},
 title = {Convolutional Neural Network Architectures for Matching Natural Language
Sentences},
 year = {2014}
}

@inproceedings{10.5555/3060832.3061030,
 author = {Shengxian Wan et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {IJCAI},
 pages = {2922--2928},
 title = {Match-SRNN: Modeling the Recursive Matching Structure with Spatial
{RNN}},
 year = {2016}
}

@inproceedings{10.5555/3304222.3304379,
 author = {Yiping Song et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {IJCAI},
 pages = {4382--4388},
 title = {An Ensemble of Retrieval-Based and Generation-Based Human-Computer
Conversation Systems},
 year = {2018}
}

@inproceedings{10.5555/3367471.3367479,
 author = {Fei Mi et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {IJCAI},
 pages = {3151--3157},
 title = {Meta-Learning for Low-resource Natural Language Generation in Task-oriented
Dialogue Systems},
 year = {2019}
}

@inproceedings{10.5555/3504035.3504121,
 author = {Amrita Saha et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {696--704},
 title = {Towards Building Large Scale Multimodal Domain-Aware Conversation
Systems},
 year = {2018}
}

@misc{2023internlm,
 author = {InternLM Team},
 howpublished = {\url{https://github.com/InternLM/InternLM}},
 title = {InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
 year = {2023}
}

@misc{23llmfactuality,
 archiveprefix = {arXiv},
 author = {Liang Chen et al.},
 eprint = {2310.07289},
 primaryclass = {cs.CL},
 title = {Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators},
 year = {2023}
}

@article{9335254,
 author = {Balaraman Vevake et al.},
 journal = {TASLP},
 pages = {866-873},
 title = {Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems},
 volume = {29},
 year = {2021}
}

@inproceedings{9948337,
 author = {Liu Guangya et al.},
 booktitle = {AEMCSE},
 pages = {845-853},
 title = {A Survey on Multimodal Dialogue Systems: Recent Advances and New Frontiers},
 volume = {},
 year = {2022}
}

@misc{ainslie2023gqa,
 archiveprefix = {arXiv},
 author = {Joshua Ainslie et al.},
 eprint = {2305.13245},
 primaryclass = {cs.CL},
 title = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
 year = {2023}
}

@inproceedings{alamri2019audiovisual,
 author = {Huda AlAmri et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {CVPR},
 pages = {7558--7567},
 title = {Audio Visual Scene-Aware Dialog},
 year = {2019}
}

@book{alice,
 author = {Wallace, Richard S},
 title = {The anatomy of ALICE},
 year = {2009}
}

@misc{alpaca,
 author = {Rohan Taori et al.},
 howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
 journal = {GitHub repository},
 title = {Stanford Alpaca: An Instruction-following LLaMA model},
 year = {2023}
}

@misc{amayuelas2023knowledge,
 archiveprefix = {arXiv},
 author = {Alfonso Amayuelas et al.},
 eprint = {2305.13712},
 primaryclass = {cs.CL},
 title = {Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models},
 year = {2023}
}

@misc{atom_chat,
 author = {llama-chinese-community},
 howpublished = {[EB/OL]},
 note = {\url{https://github.com/FlagAlpha/Llama2-Chinese} Accessed Aug , 2023},
 title = {}
}

@misc{auto_prompt,
 archiveprefix = {arXiv},
 author = {Zhuosheng Zhang et al.},
 eprint = {2210.03493},
 primaryclass = {cs.CL},
 title = {Automatic Chain of Thought Prompting in Large Language Models},
 year = {2022}
}

@misc{automatic_prompt,
 archiveprefix = {arXiv},
 author = {KaShun Shum et al.},
 eprint = {2302.12822},
 primaryclass = {cs.CL},
 title = {Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data},
 year = {2023}
}

@article{bai2022constitutional,
 author = {Bai Yuntao et al.},
 journal = {ArXiv preprint},
 title = {Constitutional ai: Harmlessness from ai feedback},
 volume = {abs/2212.08073},
 year = {2022}
}

@article{bai2022training,
 author = {Bai Yuntao et al.},
 journal = {ArXiv preprint},
 title = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
 volume = {abs/2204.05862},
 year = {2022}
}

@misc{bai2023qwen,
 archiveprefix = {arXiv},
 author = {Jinze Bai et al.},
 eprint = {2309.16609},
 primaryclass = {cs.CL},
 title = {Qwen Technical Report},
 year = {2023}
}

@inproceedings{balaraman-etal-2021-recent,
 author = {Balaraman Vevake  et al.},
 booktitle = {SIGDIAL},
 pages = {239--251},
 title = {Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey},
 year = {2021}
}

@article{bang2023multitask,
 author = {Bang Yejin et al.},
 journal = {ArXiv preprint},
 title = {A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
 volume = {abs/2302.04023},
 year = {2023}
}

@inproceedings{bao-etal-2020-plato,
 author = {Bao Siqi  et al.},
 booktitle = {ACL},
 pages = {85--96},
 title = {{PLATO}: Pre-trained Dialogue Generation Model with Discrete Latent Variable},
 year = {2020}
}

@inproceedings{bao-etal-2021-plato,
 author = {Bao Siqi  et al.},
 booktitle = {Finding. of ACL},
 pages = {2513--2525},
 title = {{PLATO-2}: Towards Building an Open-Domain Chatbot via Curriculum Learning},
 year = {2021}
}

@inproceedings{bao-etal-2022-plato,
 author = {Bao Siqi  et al.},
 booktitle = {Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022},
 pages = {107--118},
 title = {{PLATO}-{XL}: Exploring the Large-scale Pre-training of Dialogue Generation},
 year = {2022}
}

@inproceedings{barriere-etal-2022-wassa,
 author = {Barriere Valentin  et al.},
 booktitle = {Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis},
 pages = {214--227},
 title = {{WASSA} 2022 Shared Task: Predicting Empathy, Emotion and Personality in Reaction to News Stories},
 year = {2022}
}

@inproceedings{bengio2000neural,
 author = {Yoshua Bengio et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 pages = {932--938},
 title = {A Neural Probabilistic Language Model},
 year = {2000}
}

@inproceedings{blenderbot,
 author = {Roller Stephen  et al.},
 booktitle = {EACL},
 pages = {300--325},
 title = {Recipes for Building an Open-Domain Chatbot},
 year = {2021}
}

@article{bm25,
 author = {Stephen Robertson et al.},
 journal = {Foundations and Trends® in Information Retrieval},
 pages = {333-389},
 title = {The Probabilistic Relevance Framework: BM25 and Beyond},
 volume = {3},
 year = {2009}
}

@article{bobrow1977gus,
 author = {Bobrow Daniel G et al.},
 journal = {Artificial intelligence},
 pages = {155--173},
 title = {GUS, a frame-driven dialog system},
 volume = {8},
 year = {1977}
}

@inproceedings{brown2020language,
 author = {Tom B. Brown et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@inproceedings{budzianowski2018multiwoz,
 author = {Budzianowski Pawe{\l}  et al.},
 booktitle = {EMNLP},
 pages = {5016--5026},
 title = {{M}ulti{WOZ} - A Large-Scale Multi-Domain {W}izard-of-{O}z Dataset for Task-Oriented Dialogue Modelling},
 year = {2018}
}

@inproceedings{cao-etal-2022-model,
 author = {Cao Yu  et al.},
 booktitle = {ACL},
 pages = {7984--8002},
 title = {A Model-agnostic Data Manipulation Method for Persona-based Dialogue Generation},
 year = {2022}
}

@inproceedings{cao2019theoretical,
 author = {Tianshi Cao et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ICLR},
 title = {A Theoretical Analysis of the Number of Shots in Few-Shot Learning},
 year = {2020}
}

@misc{castellucci2019multilingual,
 archiveprefix = {arXiv},
 author = {Giuseppe Castellucci et al.},
 eprint = {1907.02884},
 primaryclass = {cs.CL},
 title = {Multi-lingual Intent Detection and Slot Filling in a Joint BERT-based Model},
 year = {2019}
}

@inproceedings{cdialgpt,
 author = {Wang Yida et al.},
 journal = {ArXiv preprint},
 title = {A Large-Scale Chinese Short-Text Conversation Dataset},
 volume = {abs/2008.03946},
 year = {2020}
}

@article{challenges_in_open_domain,
 abstract = {There is a resurgent interest in developing intelligent open-domain dialog systems due to the availability of large amounts of conversational data and the recent progress on neural approaches to conversational AI [33]. Unlike traditional task-oriented bots, an open-domain dialog system aims to establish long-term connections with users by satisfying the human need for communication, affection, and social belonging. This article reviews the recent work on neural approaches that are devoted to addressing three challenges in developing such systems: semantics, consistency, and interactiveness. Semantics requires a dialog system to not only understand the content of the dialog but also identify users’ emotional and social needs during the conversation. Consistency requires the system to demonstrate a consistent personality to win users’ trust and gain their long-term confidence. Interactiveness refers to the system’s ability to generate interpersonal responses to achieve particular social goals such as entertainment and conforming. The studies we select to present in this survey are based on our unique views and are by no means complete. Nevertheless, we hope that the discussion will inspire new research in developing more intelligent open-domain dialog systems.},
 articleno = {21},
 author = {Huang Minlie et al.},
 issue_date = {July 2020},
 journal = {ACM Trans. Inf. Syst.},
 keywords = {conversational AI, social bot, response generation, chatbot, Dialog system, conversation generation},
 numpages = {32},
 title = {Challenges in Building Intelligent Open-Domain Dialog Systems},
 volume = {38},
 year = {2020}
}

@article{challenges_in_tods,
 author = {Zhang Zheng et al.},
 journal = {Science China Technological Sciences},
 pages = {2011--2027},
 title = {Recent advances and challenges in task-oriented dialog systems},
 volume = {63},
 year = {2020}
}

@inproceedings{chen-etal-2019-semantically,
 author = {Chen Wenhu  et al.},
 booktitle = {ACL},
 pages = {3696--3709},
 title = {Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention},
 year = {2019}
}

@inproceedings{chen-etal-2022-ketod,
 author = {Chen Zhiyu  et al.},
 booktitle = {Finding. of NAACL},
 pages = {2581--2593},
 title = {{KETOD}: Knowledge-Enriched Task-Oriented Dialogue},
 year = {2022}
}

@article{chen2018gunrock,
 author = {Chen Chun-Yen et al.},
 journal = {Alexa prize proceedings},
 title = {Gunrock: Building a human-like social bot by leveraging large scale real user data},
 year = {2018}
}

@article{chen2019bert,
 author = {Chen Qian et al.},
 journal = {ArXiv preprint},
 title = {Bert for joint intent classification and slot filling},
 volume = {abs/1902.10909},
 year = {2019}
}

@inproceedings{chen2021dialogsum,
 author = {Chen Yulong  et al.},
 booktitle = {Finding. of ACL},
 pages = {5062--5074},
 title = {{D}ialog{S}um: {A} Real-Life Scenario Dialogue Summarization Dataset},
 year = {2021}
}

@misc{chen2021evaluating,
 archiveprefix = {arXiv},
 author = {Mark Chen et al.},
 eprint = {2107.03374},
 primaryclass = {cs.LG},
 title = {Evaluating Large Language Models Trained on Code},
 year = {2021}
}

@misc{chen2022harry,
 archiveprefix = {arXiv},
 author = {Nuo Chen et al.},
 eprint = {2211.06869},
 primaryclass = {cs.CL},
 title = {What would Harry say? Building Dialogue Agents for Characters in a Story},
 year = {2022}
}

@misc{chen2023extending,
 archiveprefix = {arXiv},
 author = {Shouyuan Chen et al.},
 eprint = {2306.15595},
 primaryclass = {cs.CL},
 title = {Extending Context Window of Large Language Models via Positional Interpolation},
 year = {2023}
}

@article{chen2023llm,
 author = {Chen Siyuan et al.},
 journal = {ArXiv preprint},
 title = {LLM-empowered Chatbots for Psychiatrist and Patient Simulation: Application and Evaluation},
 volume = {abs/2305.13614},
 year = {2023}
}

@article{chen2023palr,
 author = {Chen, Zheng},
 journal = {ArXiv preprint},
 title = {PALR: Personalization Aware LLMs for Recommendation},
 volume = {abs/2305.07622},
 year = {2023}
}

@misc{chen2023purr,
 archiveprefix = {arXiv},
 author = {Anthony Chen et al.},
 eprint = {2305.14908},
 primaryclass = {cs.CL},
 title = {PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions},
 year = {2023}
}

@misc{chen2023robust,
 archiveprefix = {arXiv},
 author = {Liang Chen et al.},
 eprint = {2305.12782},
 primaryclass = {cs.CL},
 title = {Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization},
 year = {2023}
}

@misc{chen2023xmark,
 archiveprefix = {arXiv},
 author = {Liang Chen et al.},
 eprint = {2311.09832},
 primaryclass = {cs.CL},
 title = {X-Mark: Towards Lossless Watermarking Through Lexical Redundancy},
 year = {2023}
}

@article{chen_survey_2017,
 author = {Chen Hongshen et al.},
 journal = {ACM SIGKDD Explorations Newsletter},
 pages = {25--35},
 shorttitle = {A {{Survey}} on {{Dialogue Systems}}},
 title = {A {{Survey}} on {{Dialogue Systems}}: {{Recent Advances}} and {{New Frontiers}}},
 volume = {19},
 year = {2017}
}

@misc{cheng2023pal,
 archiveprefix = {arXiv},
 author = {Jiale Cheng et al.},
 eprint = {2212.09235},
 primaryclass = {cs.CL},
 title = {PAL: Persona-Augmented Emotional Support Conversation Generation},
 year = {2023}
}

@article{chiang2023vicuna,
 author = {Chiang Wei-Lin et al.},
 journal = {See https://vicuna. lmsys. org (accessed 14 April 2023)},
 title = {Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
 year = {2023}
}

@inproceedings{chiu-etal-2022-salesbot,
 author = {Chiu Ssu  et al.},
 booktitle = {ACL},
 pages = {6143--6158},
 title = {{S}ales{B}ot: Transitioning from Chit-Chat to Task-Oriented Dialogues},
 year = {2022}
}

@inproceedings{comet,
 author = {Bosselut Antoine  et al.},
 booktitle = {ACL},
 pages = {4762--4779},
 title = {{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction},
 year = {2019}
}

@inproceedings{cosplay,
 author = {Chen Xu et al.},
 booktitle = {SIGIR},
 title = {{COSPLAY}},
 year = {2022}
}

@misc{cot,
 archiveprefix = {arXiv},
 author = {Jason Wei et al.},
 eprint = {2201.11903},
 primaryclass = {cs.CL},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 year = {2023}
}

@misc{cuecot,
 archiveprefix = {arXiv},
 author = {Hongru Wang et al.},
 eprint = {2305.11792},
 primaryclass = {cs.CL},
 title = {Chain-of-thought prompting for responding to in-depth dialogue questions with LLM},
 year = {2023}
}

@inproceedings{d4,
 author = {Yao Binwei  et al.},
 booktitle = {EMNLP},
 pages = {2438--2459},
 title = {D4: a {C}hinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat},
 year = {2022}
}

@misc{dai2023instructblip,
 archiveprefix = {arXiv},
 author = {Wenliang Dai et al.},
 eprint = {2305.06500},
 primaryclass = {cs.CV},
 title = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
 year = {2023}
}

@software{damo2022plug,
 author = {Wei Wang et al.},
 title = {PLUG：Pre-training for Language Understanding and Generation},
 year = {2022}
}

@misc{dao2022flashattention,
 archiveprefix = {arXiv},
 author = {Tri Dao et al.},
 eprint = {2205.14135},
 primaryclass = {cs.LG},
 title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
 year = {2022}
}

@inproceedings{das2017visual,
 author = {Abhishek Das et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {CVPR},
 pages = {1080--1089},
 title = {Visual Dialog},
 year = {2017}
}

@inproceedings{DBLP:conf/aaai/YoungXPNC22,
 author = {Tom Young et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {11622--11629},
 title = {Fusing Task-Oriented and Open-Domain Dialogues in Conversational Agents},
 year = {2022}
}

@inproceedings{DBLP:conf/ijcai/0002LLC23,
 author = {Yang Deng et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {IJCAI},
 pages = {6583--6591},
 title = {A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects},
 year = {2023}
}

@article{DBLP:journals/corr/abs-2309-11838,
 author = {Norbert Braunschweiler et al.},
 journal = {ArXiv preprint},
 title = {Evaluating Large Language Models for Document-grounded Response Generation
in Information-Seeking Dialogues},
 volume = {abs/2309.11838},
 year = {2023}
}

@misc{deng2023prompting,
 archiveprefix = {arXiv},
 author = {Yang Deng et al.},
 eprint = {2305.13626},
 primaryclass = {cs.CL},
 title = {Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration},
 year = {2023}
}

@article{deng2023recent,
 author = {Deng Jiawen et al.},
 journal = {ArXiv preprint},
 title = {Recent advances towards safe, responsible, and moral dialogue systems: A survey},
 volume = {abs/2302.09270},
 year = {2023}
}

@article{deshpande2023toxicity,
 author = {Deshpande Ameet et al.},
 journal = {ArXiv preprint},
 title = {Toxicity in chatgpt: Analyzing persona-assigned language models},
 volume = {abs/2304.05335},
 year = {2023}
}

@inproceedings{devlin-etal-2019-bert,
 author = {Devlin Jacob  et al.},
 booktitle = {NAACL-HLT},
 pages = {4171--4186},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

@inproceedings{dinan2022safetykit,
 author = {Dinan Emily  et al.},
 booktitle = {ACL},
 pages = {4113--4133},
 title = {{S}afety{K}it: First Aid for Measuring Safety in Open-domain Conversational Systems},
 year = {2022}
}

@misc{ding2023longnet,
 archiveprefix = {arXiv},
 author = {Jiayu Ding et al.},
 eprint = {2307.02486},
 primaryclass = {cs.CL},
 title = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
 year = {2023}
}

@inproceedings{dpr,
 author = {Karpukhin Vladimir  et al.},
 booktitle = {EMNLP},
 pages = {6769--6781},
 title = {Dense Passage Retrieval for Open-Domain Question Answering},
 year = {2020}
}

@inproceedings{dst_survey,
 author = {Jacqmin L{\'e}o  et al.},
 booktitle = {SIGDIAL},
 pages = {336--350},
 title = {{``}Do you follow me?{''}: A Survey of Recent Approaches in Dialogue State Tracking},
 year = {2022}
}

@inproceedings{du2022glm,
 author = {Du Zhengxiao  et al.},
 booktitle = {ACL},
 pages = {320--335},
 title = {{GLM}: General Language Model Pretraining with Autoregressive Blank Infilling},
 year = {2022}
}

@inproceedings{dulemon,
 author = {Xu Xinchao  et al.},
 booktitle = {Finding. of ACL},
 pages = {2639--2650},
 title = {Long Time No See! Open-Domain Conversation with Long-Term Persona Memory},
 year = {2022}
}

@inproceedings{dziri-etal-2021-neural,
 author = {Dziri Nouha  et al.},
 booktitle = {EMNLP},
 pages = {2197--2214},
 title = {Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding},
 year = {2021}
}

@inproceedings{ed,
 author = {Rashkin Hannah  et al.},
 booktitle = {ACL},
 pages = {5370--5381},
 title = {Towards Empathetic Open-domain Conversation Models: A New Benchmark and Dataset},
 year = {2019}
}

@inproceedings{ekman1971universals,
 author = {Ekman, Paul},
 booktitle = {Nebraska symposium on motivation},
 title = {Universals and cultural differences in facial expressions of emotion.},
 year = {1971}
}

@article{electronics11203409,
 abstract = {The recent advancements in multimodal dialogue systems have been gaining importance in several domains such as retail, travel, fashion, among others. Several existing works have improved the understanding and generation of multimodal dialogues. However, there still exists considerable space to improve the quality of output textual responses due to insufficient information infusion between the visual and textual semantics. Moreover, the existing dialogue systems often generate defective knowledge-aware responses for tasks such as providing product attributes and celebrity endorsements. To address the aforementioned issues, we present a Transformer-based Multimodal Infusion Dialogue (TMID) system that extracts the visual and textual information from dialogues via a transformer-based multimodal context encoder and employs a cross-attention mechanism to achieve information infusion between images and texts for each utterance. Furthermore, TMID uses adaptive decoders to generate appropriate multimodal responses based on the user intentions it has determined using a state classifier and enriches the output responses by incorporating domain knowledge into the decoders. The results of extensive experiments on a multimodal dialogue dataset demonstrate that TMID has achieved a state-of-the-art performance by improving the BLUE-4 score by 13.03, NIST by 2.77, image selection Recall@1 by 1.84%.},
 article-number = {3409},
 author = {Liu Bo et al.},
 journal = {Electronics},
 title = {Transformer-Based Multimodal Infusion Dialogue Systems},
 volume = {11},
 year = {2022}
}

@inproceedings{emh,
 author = {Sharma Ashish  et al.},
 booktitle = {EMNLP},
 pages = {5263--5276},
 title = {A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support},
 year = {2020}
}

@inproceedings{eric-etal-2020-multiwoz,
 author = {Eric Mihail  et al.},
 booktitle = {LREC},
 isbn = {979-10-95546-34-4},
 language = {English},
 pages = {422--428},
 title = {{M}ulti{WOZ} 2.1: A Consolidated Multi-Domain Dialogue Dataset with State Corrections and State Tracking Baselines},
 year = {2020}
}

@misc{fan2023uncovering,
 archiveprefix = {arXiv},
 author = {Yaxin Fan et al.},
 eprint = {2305.08391},
 primaryclass = {cs.CL},
 title = {Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study},
 year = {2023}
}

@inproceedings{feng-etal-2023-mmdialog,
 abstract = {Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 88x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset is available in \url{https://github.com/victorsungo/MMDialog}.},
 author = {Feng Jiazhan  et al.},
 booktitle = {Proc. of ACL},
 pages = {7348--7363},
 title = {{MMD}ialog: A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation},
 year = {2023}
}

@article{fewshot_survey,
 author = {Hospedales Timothy et al.},
 journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
 pages = {5149-5169},
 title = {Meta-Learning in Neural Networks: A Survey},
 volume = {44},
 year = {2022}
}

@inproceedings{focus,
 author = {Yoonna Jang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {10803--10812},
 title = {Call for Customized Conversation: Customized Conversation Grounding
Persona and Knowledge},
 year = {2022}
}

@inproceedings{fu-etal-2022-thousand,
 author = {Fu Tingchen  et al.},
 booktitle = {ACL},
 pages = {3901--3913},
 title = {There Are a Thousand Hamlets in a Thousand People{'}s Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory},
 year = {2022}
}

@article{fu2023reasoning,
 author = {Fu Yahui et al.},
 journal = {ArXiv preprint},
 title = {Reasoning before Responding: Integrating Commonsense-based Causality Explanation for Empathetic Response Generation},
 volume = {abs/2308.00085},
 year = {2023}
}

@article{ganguli2022red,
 author = {Ganguli Deep et al.},
 journal = {ArXiv preprint},
 title = {Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
 volume = {abs/2209.07858},
 year = {2022}
}

@inproceedings{gao-etal-2018-neural-approaches,
 author = {Jianfeng Gao et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {SIGIR},
 pages = {1371--1374},
 title = {Neural Approaches to Conversational {AI}},
 year = {2018}
}

@inproceedings{gao-etal-2020-dialogue,
 author = {Gao Xiang  et al.},
 booktitle = {EMNLP},
 pages = {386--395},
 title = {Dialogue Response Ranking Training with Large-Scale Human Feedback Data},
 year = {2020}
}

@inproceedings{gao-etal-2021-hyknow,
 author = {Gao Silin  et al.},
 booktitle = {Finding. of ACL},
 pages = {1591--1602},
 title = {{H}y{K}now: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge Management},
 year = {2021}
}

@misc{gao2023enabling,
 archiveprefix = {arXiv},
 author = {Tianyu Gao et al.},
 eprint = {2305.14627},
 primaryclass = {cs.CL},
 title = {Enabling Large Language Models to Generate Text with Citations},
 year = {2023}
}

@inproceedings{gao_dialog_2019,
 author = {Gao Shuyang  et al.},
 booktitle = {SIGDIAL},
 pages = {264--273},
 title = {Dialog State Tracking: A Neural Reading Comprehension Approach},
 year = {2019}
}

@inproceedings{gdpl,
 author = {Takanobu Ryuichi  et al.},
 booktitle = {EMNLP},
 pages = {100--110},
 title = {Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog},
 year = {2019}
}

@inproceedings{gerz-etal-2021-multilingual,
 author = {Gerz Daniela  et al.},
 booktitle = {EMNLP},
 pages = {7468--7475},
 title = {Multilingual and Cross-Lingual Intent Detection from Spoken Data},
 year = {2021}
}

@inproceedings{ghazvininejad2018knowledge,
 author = {Marjan Ghazvininejad et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {5110--5117},
 title = {A Knowledge-Grounded Neural Conversation Model},
 year = {2018}
}

@inproceedings{ghosal-etal-2020-cosmic,
 author = {Ghosal Deepanway  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {2470--2481},
 title = {{COSMIC}: {CO}mmon{S}ense knowledge for e{M}otion Identification in Conversations},
 year = {2020}
}

@inproceedings{ghosh-etal-2022-team,
 author = {Ghosh Soumitra  et al.},
 booktitle = {Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis},
 pages = {255--260},
 title = {Team {IITP}-{AINLPML} at {WASSA} 2022: Empathy Detection, Emotion Classification and Personality Detection},
 year = {2022}
}

@misc{girdhar2023imagebind,
 archiveprefix = {arXiv},
 author = {Rohit Girdhar et al.},
 eprint = {2305.05665},
 primaryclass = {cs.CV},
 title = {ImageBind: One Embedding Space To Bind Them All},
 year = {2023}
}

@inproceedings{goo2018slot,
 author = {Goo Chih-Wen  et al.},
 booktitle = {NAACL-HLT},
 pages = {753--757},
 title = {Slot-Gated Modeling for Joint Slot Filling and Intent Prediction},
 year = {2018}
}

@article{gpt,
 author = {Radford Alec et al.},
 title = {Improving language understanding by generative pre-training},
 year = {2018}
}

@article{gpt2,
 author = {Radford Alec et al.},
 journal = {OpenAI blog},
 pages = {9},
 title = {Language models are unsupervised multitask learners},
 volume = {1},
 year = {2019}
}

@misc{gpt4,
 author = {OpenAI},
 howpublished = {[EB/OL]},
 note = {\url{https://cdn.openai.com/papers/gpt-4.pdf} Accessed Mar 27, 2023},
 title = {GPT-4 Technical Report}
}

@inproceedings{gru,
 author = {Cho Kyunghyun  et al.},
 booktitle = {EMNLP},
 pages = {1724--1734},
 title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
 year = {2014}
}

@inproceedings{gu2020speaker,
 author = {Jia{-}Chen Gu et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {{CIKM} '20: The 29th {ACM} International Conference on Information
and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020},
 pages = {2041--2044},
 title = {Speaker-Aware {BERT} for Multi-Turn Response Selection in Retrieval-Based
Chatbots},
 year = {2020}
}

@misc{gu2022eva20,
 archiveprefix = {arXiv},
 author = {Yuxian Gu et al.},
 eprint = {2203.09313},
 primaryclass = {cs.CL},
 title = {EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training},
 year = {2022}
}

@inproceedings{guo-etal-2022-beyond,
 author = {Guo Jinyu  et al.},
 booktitle = {ACL},
 pages = {2320--2332},
 title = {Beyond the Granularity: Multi-Perspective Dialogue Collaborative Selection for Dialogue State Tracking},
 year = {2022}
}

@article{guo-etal-2023-hc3,
 author = {Guo Biyang  et al.},
 journal = {ArXiv preprint},
 title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},
 volume = {abs/2301.07597},
 year = {2023}
}

@inproceedings{gururangan-etal-2020-dont,
 author = {Gururangan Suchin  et al.},
 booktitle = {ACL},
 pages = {8342--8360},
 title = {Don{'}t Stop Pretraining: Adapt Language Models to Domains and Tasks},
 year = {2020}
}

@inproceedings{hakkani2016multi,
 author = {Dilek Hakkani{-}T{\"{u}}r et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {INTERSPEECH},
 pages = {715--719},
 title = {Multi-Domain Joint Semantic Frame Parsing Using Bi-Directional {RNN-LSTM}},
 year = {2016}
}

@article{halueval,
 author = {Junyi Li et al.},
 journal = {ArXiv preprint},
 title = {HaluEval: {A} Large-Scale Hallucination Evaluation Benchmark for Large
Language Models},
 volume = {abs/2305.11747},
 year = {2023}
}

@inproceedings{ham-etal-2020-end,
 author = {Ham Donghoon  et al.},
 booktitle = {ACL},
 pages = {583--592},
 title = {End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using {GPT}-2},
 year = {2020}
}

@inproceedings{he2022galaxy,
 author = {Wanwei He et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {10749--10757},
 title = {{GALAXY:} {A} Generative Pre-trained Model for Task-Oriented Dialog
with Semi-supervised Learning and Explicit Policy Injection},
 year = {2022}
}

@inproceedings{he2022unified,
 author = {He Wanwei et al.},
 booktitle = {SIGIR},
 pages = {187--200},
 title = {Unified dialog model pre-training for task-oriented dialog understanding and generation},
 year = {2022}
}

@article{he2023can,
 author = {He Mutian et al.},
 journal = {ArXiv preprint},
 title = {Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding},
 volume = {abs/2305.13512},
 year = {2023}
}

@inproceedings{heck-etal-2020-trippy,
 author = {Heck Michael  et al.},
 booktitle = {SIGDIAL},
 pages = {35--44},
 title = {{T}rip{P}y: A Triple Copy Strategy for Value Independent Neural Dialog State Tracking},
 year = {2020}
}

@inproceedings{heck-etal-2023-chatgpt,
 abstract = {Recent research on dialog state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT research preview, showing that ChatGPT achieves state-of-the-art performance in zero-shot DST. Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems. We further theorize that the in-context learning capabilities of such models will likely become powerful tools to support the development of dedicated dialog state trackers and enable dynamic methods.},
 author = {Heck Michael  et al.},
 booktitle = {Proc. of ACL},
 pages = {936--950},
 title = {{C}hat{GPT} for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?},
 year = {2023}
}

@inproceedings{henderson-etal-2014-second,
 author = {Henderson Matthew  et al.},
 booktitle = {SIGDIAL},
 pages = {263--272},
 title = {The Second Dialog State Tracking Challenge},
 year = {2014}
}

@inproceedings{henderson-etal-2014-word,
 author = {Henderson Matthew  et al.},
 booktitle = {SIGDIAL},
 pages = {292--299},
 title = {Word-Based Dialog State Tracking with Recurrent Neural Networks},
 year = {2014}
}

@inproceedings{henderson-etal-2020-convert,
 author = {Henderson Matthew  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {2161--2174},
 title = {{C}onve{RT}: Efficient and Accurate Conversational Representations from Transformers},
 year = {2020}
}

@article{henderson_hybrid_2008,
 author = {Henderson James  et al.},
 journal = {Computational Linguistics},
 pages = {487--511},
 title = {Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets},
 volume = {34},
 year = {2008}
}

@inproceedings{hosseini2020simple,
 author = {Ehsan Hosseini{-}Asl et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 title = {A Simple Language Model for Task-Oriented Dialogue},
 year = {2020}
}

@inproceedings{hou2020fewshot,
 author = {Hou Yutai  et al.},
 booktitle = {ACL},
 pages = {1381--1393},
 title = {Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network},
 year = {2020}
}

@inproceedings{hough-etal-2016-duel,
 author = {Hough Julian  et al.},
 booktitle = {LREC},
 pages = {1784--1788},
 title = {{DUEL}: A Multi-lingual Multimodal Dialogue Corpus for Disfluency, Exclamations and Laughter},
 year = {2016}
}

@inproceedings{hron2020infinite,
 author = {Jiri Hron et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ICML},
 pages = {4376--4386},
 series = {Proceedings of Machine Learning Research},
 title = {Infinite attention: {NNGP} and {NTK} for deep attention networks},
 volume = {119},
 year = {2020}
}

@article{hsu2023helping,
 author = {Hsu Shang-Ling et al.},
 journal = {ArXiv preprint},
 title = {Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback},
 volume = {abs/2305.08982},
 year = {2023}
}

@inproceedings{hu-etal-2020-sas,
 author = {Hu Jiaying  et al.},
 booktitle = {ACL},
 pages = {6366--6375},
 title = {{SAS}: Dialogue State Tracking via Slot Attention and Slot Information Sharing},
 year = {2020}
}

@inproceedings{hu2021lora,
 author = {Edward J. Hu et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ICLR},
 title = {LoRA: Low-Rank Adaptation of Large Language Models},
 year = {2022}
}

@article{hu2023chatdb,
 author = {Hu Chenxu et al.},
 journal = {ArXiv preprint},
 title = {ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory},
 volume = {abs/2306.03901},
 year = {2023}
}

@inproceedings{huang2022language,
 author = {Wenlong Huang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 pages = {9118--9147},
 series = {Proceedings of Machine Learning Research},
 title = {Language Models as Zero-Shot Planners: Extracting Actionable Knowledge
for Embodied Agents},
 volume = {162},
 year = {2022}
}

@misc{Huang_hallucination_survey,
 author = {Huang Yichong et al.},
 journal = {ArXiv preprint},
 title = {The Factual Inconsistency Problem in Abstractive Text Summarization: A Survey},
 volume = {abs/2104.14839},
 year = {2021}
}

@inproceedings{hudevcek2023large,
 author = {Hude{\v{c}}ek Vojt{\v{e}}ch et al.},
 booktitle = {SIGDIAL},
 pages = {216--228},
 title = {Are Large Language Models All You Need for Task-Oriented Dialogue?},
 year = {2023}
}

@article{hupkes2020compositionality,
 author = {Hupkes Dieuwke et al.},
 journal = {JAIR},
 pages = {757--795},
 title = {Compositionality decomposed: How do neural networks generalise?},
 volume = {67},
 year = {2020}
}

@misc{huynh2023understanding,
 archiveprefix = {arXiv},
 author = {Jessica Huynh et al.},
 eprint = {2301.12004},
 primaryclass = {cs.CL},
 title = {Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation},
 year = {2023}
}

@misc{instructgpt,
 archiveprefix = {arXiv},
 author = {Long Ouyang et al.},
 eprint = {2203.02155},
 primaryclass = {cs.CL},
 title = {Training language models to follow instructions with human feedback},
 year = {2022}
}

@article{irving2019ai,
 author = {Irving Geoffrey et al.},
 journal = {Distill},
 pages = {e14},
 title = {AI safety needs social scientists},
 volume = {4},
 year = {2019}
}

@misc{jannai2023human,
 archiveprefix = {arXiv},
 author = {Daniel Jannai et al.},
 eprint = {2305.20010},
 primaryclass = {cs.AI},
 title = {Human or Not? A Gamified Approach to the Turing Test},
 year = {2023}
}

@article{ji2023exploring,
 author = {Ji Yunjie et al.},
 journal = {ArXiv preprint},
 title = {Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases},
 volume = {abs/2303.14742},
 year = {2023}
}

@article{ji2023survey,
 author = {Ji Ziwei et al.},
 journal = {ACM Computing Surveys},
 pages = {1--38},
 title = {Survey of hallucination in natural language generation},
 volume = {55},
 year = {2023}
}

@inproceedings{johnston-etal-2002-match,
 author = {Johnston Michael  et al.},
 booktitle = {ACL},
 pages = {376--383},
 title = {{MATCH}: An Architecture for Multimodal Dialogue Systems},
 year = {2002}
}

@misc{kandpal2023large,
 archiveprefix = {arXiv},
 author = {Nikhil Kandpal et al.},
 eprint = {2211.08411},
 primaryclass = {cs.CL},
 title = {Large Language Models Struggle to Learn Long-Tail Knowledge},
 year = {2023}
}

@misc{kaplan2020scaling,
 archiveprefix = {arXiv},
 author = {Jared Kaplan et al.},
 eprint = {2001.08361},
 primaryclass = {cs.LG},
 title = {Scaling Laws for Neural Language Models},
 year = {2020}
}

@article{kasirzadeh2023conversation,
 author = {Kasirzadeh Atoosa et al.},
 journal = {Philosophy \& Technology},
 pages = {1--24},
 title = {In conversation with Artificial Intelligence: aligning language models with human values},
 volume = {36},
 year = {2023}
}

@inproceedings{kim-etal-2020-beyond,
 author = {Kim Seokhwan  et al.},
 booktitle = {SIGDIAL},
 pages = {278--289},
 title = {Beyond Domain {API}s: Task-oriented Conversational Modeling with Unstructured Knowledge Access},
 year = {2020}
}

@inproceedings{kim-etal-2020-efficient,
 author = {Kim Sungdong  et al.},
 booktitle = {ACL},
 pages = {567--582},
 title = {Efficient Dialogue State Tracking by Selectively Overwriting Memory},
 year = {2020}
}

@article{knowledge-survey,
 author = {Wenhao Yu et al.},
 journal = {{ACM} Comput. Surv.},
 pages = {227:1--227:38},
 title = {A Survey of Knowledge-enhanced Text Generation},
 volume = {54},
 year = {2022}
}

@inproceedings{komeili-etal-2022-internet,
 author = {Komeili Mojtaba  et al.},
 booktitle = {ACL},
 pages = {8460--8478},
 title = {{I}nternet-Augmented Dialogue Generation},
 year = {2022}
}

@inproceedings{kottur2017exploring,
 author = {Satwik Kottur et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {IJCAI},
 pages = {3728--3734},
 title = {Exploring Personalized Neural Conversational Models},
 year = {2017}
}

@inproceedings{kulhanek-etal-2021-augpt,
 author = {Kulh{\'a}nek Jon{\'a}{\v{s}}  et al.},
 booktitle = {Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI},
 pages = {198--210},
 title = {{AuGPT}: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models},
 year = {2021}
}

@inproceedings{lee-2021-improving-end,
 author = {Lee, Yohan},
 booktitle = {Finding. of EMNLP},
 pages = {1296--1303},
 title = {Improving End-to-End Task-Oriented Dialog System with A Simple Auxiliary Task},
 year = {2021}
}

@inproceedings{lee-etal-2019-sumbt,
 author = {Lee Hwaran  et al.},
 booktitle = {ACL},
 pages = {5478--5483},
 title = {{SUMBT}: Slot-Utterance Matching for Universal and Scalable Belief Tracking},
 year = {2019}
}

@misc{lee2023factuality,
 archiveprefix = {arXiv},
 author = {Nayeon Lee et al.},
 eprint = {2206.04624},
 primaryclass = {cs.CL},
 title = {Factuality Enhanced Language Models for Open-Ended Text Generation},
 year = {2023}
}

@article{lee2023prompted,
 author = {Lee Gibbeum et al.},
 journal = {ArXiv preprint},
 title = {Prompted LLMs as Chatbot Modules for Long Open-domain Conversation},
 volume = {abs/2305.04533},
 year = {2023}
}

@inproceedings{levin_learning_1997,
 author = {Levin E. et al.},
 booktitle = {1997 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding} {Proceedings}},
 keywords = {dialog, Design optimization, History, State-space methods, dpl, Databases, Speech recognition, Natural languages, Hidden Markov models, Information systems, Learning, Stochastic systems},
 pages = {72--79},
 title = {Learning dialogue strategies within the {Markov} decision process framework},
 year = {1997}
}

@inproceedings{li-etal-2020-slot,
 author = {Li Yangming  et al.},
 booktitle = {ACL},
 pages = {97--106},
 title = {Slot-consistent {NLG} for Task-oriented Dialogue Systems with Iterative Rectification Network},
 year = {2020}
}

@inproceedings{li2016persona,
 author = {Li Jiwei  et al.},
 booktitle = {ACL},
 pages = {994--1003},
 title = {A Persona-Based Neural Conversation Model},
 year = {2016}
}

@inproceedings{li2021interpretable,
 author = {Yangming Li et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {13306--13314},
 title = {Interpretable {NLG} for Task-oriented Dialogue Systems with Heterogeneous
Rendering Machines},
 year = {2021}
}

@misc{li2023apibank,
 archiveprefix = {arXiv},
 author = {Minghao Li et al.},
 eprint = {2304.08244},
 primaryclass = {cs.CL},
 title = {API-Bank: A Benchmark for Tool-Augmented LLMs},
 year = {2023}
}

@misc{li2023large,
 archiveprefix = {arXiv},
 author = {Cheng Li et al.},
 eprint = {2307.11760},
 primaryclass = {cs.CL},
 title = {Large Language Models Understand and Can be Enhanced by Emotional Stimuli},
 year = {2023}
}

@article{li2023m3it,
 author = {Lei Li et al.},
 journal = {ArXiv preprint},
 title = {M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning},
 volume = {abs/2306.04387},
 year = {2023}
}

@article{li_guided_2020,
 author = {Li Ziming et al.},
 journal = {ArXiv preprint},
 title = {Guided {Dialog} {Policy} {Learning} without {Adversarial} {Learning} in the {Loop}},
 volume = {abs/2004.03267},
 year = {2020}
}

@misc{liang2023taskmatrixai,
 archiveprefix = {arXiv},
 author = {Yaobo Liang et al.},
 eprint = {2303.16434},
 primaryclass = {cs.AI},
 title = {TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs},
 year = {2023}
}

@inproceedings{lin-2004-rouge,
 author = {Lin, Chin-Yew},
 booktitle = {Text Summarization Branches Out},
 pages = {74--81},
 title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
 year = {2004}
}

@inproceedings{lin-etal-2020-mintl,
 author = {Lin Zhaojiang  et al.},
 booktitle = {EMNLP},
 pages = {3391--3405},
 title = {{M}in{TL}: Minimalist Transfer Learning for Task-Oriented Dialogue Systems},
 year = {2020}
}

@inproceedings{lin-etal-2020-world,
 author = {Lin Zibo  et al.},
 booktitle = {EMNLP},
 pages = {9220--9229},
 title = {The World is Not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection},
 year = {2020}
}

@inproceedings{lin2022truthfulqa,
 author = {Lin Stephanie  et al.},
 booktitle = {ACL},
 pages = {3214--3252},
 title = {{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods},
 year = {2022}
}

@misc{lin2023mpt,
 archiveprefix = {arXiv},
 author = {Kevin Lin et al.},
 eprint = {2211.13357},
 primaryclass = {cs.CV},
 title = {MPT: Mesh Pre-Training with Transformers for Human Pose and Mesh Reconstruction},
 year = {2023}
}

@inproceedings{liu-etal-2020-impress,
 author = {Liu Qian  et al.},
 booktitle = {ACL},
 pages = {1417--1427},
 title = {You Impress Me: Dialogue Generation via Mutual Persona Perception},
 year = {2020}
}

@inproceedings{liu-etal-2022-multi,
 author = {Liu Zihan  et al.},
 booktitle = {Finding. of ACL},
 pages = {1317--1337},
 title = {Multi-Stage Prompting for Knowledgeable Dialogue Generation},
 year = {2022}
}

@inproceedings{liu-lane-2018-adversarial,
 author = {Liu Bing  et al.},
 booktitle = {Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue},
 pages = {350--359},
 title = {Adversarial Learning of Task-Oriented Neural Dialog Models},
 year = {2018}
}

@inproceedings{liu2016attention,
 author = {Bing Liu et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {INTERSPEECH},
 pages = {685--689},
 title = {Attention-Based Recurrent Neural Network Models for Joint Intent Detection
and Slot Filling},
 year = {2016}
}

@inproceedings{liu2021emotional,
 author = {Liu Siyang  et al.},
 booktitle = {ACL},
 pages = {3469--3483},
 title = {Towards Emotional Support Dialog Systems},
 year = {2021}
}

@misc{liu2023chatcounselor,
 archiveprefix = {arXiv},
 author = {June M. Liu et al.},
 eprint = {2309.15461},
 primaryclass = {cs.CL},
 title = {ChatCounselor: A Large Language Models for Mental Health Support},
 year = {2023}
}

@misc{liu2023evaluating,
 archiveprefix = {arXiv},
 author = {Nelson F. Liu et al.},
 eprint = {2304.09848},
 primaryclass = {cs.CL},
 title = {Evaluating Verifiability in Generative Search Engines},
 year = {2023}
}

@article{liu2023gpt,
 author = {Liu Xiao et al.},
 journal = {AI Open},
 title = {GPT understands, too},
 year = {2023}
}

@article{liu2023jailbreaking,
 author = {Liu Yi et al.},
 journal = {ArXiv preprint},
 title = {Jailbreaking chatgpt via prompt engineering: An empirical study},
 volume = {abs/2305.13860},
 year = {2023}
}

@misc{liu2023visual,
 archiveprefix = {arXiv},
 author = {Haotian Liu et al.},
 eprint = {2304.08485},
 primaryclass = {cs.CV},
 title = {Visual Instruction Tuning},
 year = {2023}
}

@article{liu2023webglm,
 author = {Liu Xiao et al.},
 journal = {ArXiv preprint},
 title = {WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences},
 volume = {abs/2306.07906},
 year = {2023}
}

@misc{llm_survey,
 archiveprefix = {arXiv},
 author = {Ce Zhou et al.},
 eprint = {2302.09419},
 primaryclass = {cs.AI},
 title = {A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT},
 year = {2023}
}

@misc{logankilpatrick-2023,
 author = {logankilpatrick},
 howpublished = {\url{https://github.com/openai/openai-python/blob/main/chatml.md}},
 note = {2023-10-19},
 title = {OpenAI Python Library}
}

@article{lstm,
 author = {Hochreiter Sepp et al.},
 journal = {Neural computation},
 pages = {1735--1780},
 title = {Long short-term memory},
 volume = {9},
 year = {1997}
}

@inproceedings{lu2020improving,
 author = {Junyu Lu et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {SIGIR},
 pages = {1805--1808},
 title = {Improving Contextual Language Models for Response Retrieval in Multi-Turn
Conversation},
 year = {2020}
}

@article{lu2023memochat,
 author = {Lu Junru et al.},
 journal = {ArXiv preprint},
 title = {MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation},
 volume = {abs/2308.08239},
 year = {2023}
}

@inproceedings{ma-etal-2022-unitranser,
 author = {Ma Zhiyuan  et al.},
 booktitle = {ACL},
 pages = {103--114},
 title = {{U}ni{T}ran{S}e{R}: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System},
 year = {2022}
}

@misc{maaz2023videochatgpt,
 archiveprefix = {arXiv},
 author = {Muhammad Maaz et al.},
 eprint = {2306.05424},
 primaryclass = {cs.CV},
 title = {Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
 year = {2023}
}

@misc{madaan2023selfrefine,
 archiveprefix = {arXiv},
 author = {Aman Madaan et al.},
 eprint = {2303.17651},
 primaryclass = {cs.CL},
 title = {Self-Refine: Iterative Refinement with Self-Feedback},
 year = {2023}
}

@inproceedings{madotto-etal-2019-personalizing,
 author = {Madotto Andrea  et al.},
 booktitle = {ACL},
 pages = {5454--5459},
 title = {Personalizing Dialogue Agents via Meta-Learning},
 year = {2019}
}

@inproceedings{madotto2020adapterbot,
 author = {Zhaojiang Lin et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {16081--16083},
 title = {The Adapter-Bot: All-In-One Controllable Conversational Model},
 year = {2021}
}

@inproceedings{mairesse-etal-2010-phrase,
 author = {Mairesse Fran{\c{c}}ois  et al.},
 booktitle = {ACL},
 pages = {1552--1561},
 title = {Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning},
 year = {2010}
}

@article{mairesse2007using,
 author = {Mairesse Fran{\c{c}}ois et al.},
 journal = {JAIR},
 pages = {457--500},
 title = {Using linguistic cues for the automatic recognition of personality in conversation and text},
 volume = {30},
 year = {2007}
}

@inproceedings{majumder-etal-2020-like,
 author = {Majumder Bodhisattwa Prasad  et al.},
 booktitle = {EMNLP},
 pages = {9194--9206},
 title = {Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions},
 year = {2020}
}

@article{mao2023large,
 author = {Mao Kelong et al.},
 journal = {ArXiv preprint},
 title = {Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search},
 volume = {abs/2303.06573},
 year = {2023}
}

@inproceedings{mazare-etal-2018-training,
 author = {Mazar{\'e} Pierre-Emmanuel  et al.},
 booktitle = {EMNLP},
 pages = {2775--2779},
 title = {Training Millions of Personalized Dialogue Agents},
 year = {2018}
}

@misc{meena,
 archiveprefix = {arXiv},
 author = {Daniel Adiwardana et al.},
 eprint = {2001.09977},
 primaryclass = {cs.CL},
 title = {Towards a Human-like Open-Domain Chatbot},
 year = {2020}
}

@inproceedings{mei2022mitigating,
 author = {Mei Alex  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {2914--2926},
 title = {Mitigating Covertly Unsafe Text within Natural Language Systems},
 year = {2022}
}

@misc{mendonça2023simple,
 archiveprefix = {arXiv},
 author = {John Mendonça et al.},
 eprint = {2308.16797},
 primaryclass = {cs.CL},
 title = {Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation},
 year = {2023}
}

@article{mesnil2014using,
 author = {Mesnil Gr{\'e}goire et al.},
 journal = {TASLP},
 pages = {530--539},
 title = {Using recurrent neural networks for slot filling in spoken language understanding},
 volume = {23},
 year = {2014}
}

@article{mi2022pangu,
 author = {Mi Fei et al.},
 journal = {ArXiv preprint},
 title = {PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model},
 volume = {abs/2203.17090},
 year = {2022}
}

@misc{mi2022pangubot,
 archiveprefix = {arXiv},
 author = {Fei Mi et al.},
 eprint = {2203.17090},
 primaryclass = {cs.CL},
 title = {PanGu-Bot: Efficient Generative Dialogue Pre-training from Pre-trained Language Model},
 year = {2022}
}

@inproceedings{mikolov2010recurrent,
 author = {Mikolov Tomas et al.},
 booktitle = {Interspeech},
 pages = {1045--1048},
 title = {Recurrent neural network based language model.},
 volume = {2},
 year = {2010}
}

@misc{min2023factscore,
 archiveprefix = {arXiv},
 author = {Sewon Min et al.},
 eprint = {2305.14251},
 primaryclass = {cs.CL},
 title = {FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation},
 year = {2023}
}

@inproceedings{mine_icassp,
 author = {Wang Hongru et al.},
 booktitle = {ICASSP},
 pages = {6692--6696},
 title = {Integrating Pretrained Language Model for Dialogue Policy Evaluation},
 year = {2022}
}

@article{mine_mir,
 author = {Kwan Wai-Chung et al.},
 journal = {Machine Intelligence Research},
 pages = {1--17},
 title = {A Survey on Recent Advances and Challenges in Reinforcement Learning Methods for Task-oriented Dialogue Policy Learning},
 year = {2023}
}

@article{minigpt4-v1,
 author = {Deyao Zhu et al.},
 journal = {ArXiv preprint},
 title = {MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large
Language Models},
 volume = {abs/2304.10592},
 year = {2023}
}

@article{minigpt5,
 author = {Kaizhi Zheng et al.},
 journal = {ArXiv preprint},
 title = {MiniGPT-5: Interleaved Vision-and-Language Generation via Generative
Vokens},
 volume = {abs/2310.02239},
 year = {2023}
}

@inproceedings{mintl,
 author = {Lin Zhaojiang  et al.},
 booktitle = {EMNLP},
 pages = {3391--3405},
 title = {{M}in{TL}: Minimalist Transfer Learning for Task-Oriented Dialogue Systems},
 year = {2020}
}

@inproceedings{more_is_better,
 author = {Wu Sixing  et al.},
 booktitle = {EMNLP},
 pages = {2286--2300},
 title = {More is Better: Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge},
 year = {2021}
}

@online{MosaicML2023Introducing,
 author = {MosaicML NLP Team},
 note = {Accessed: 2023-05-05},
 title = {Introducing MPT-7B: A New Standard for Open-Source,
Commercially Usable LLMs},
 urldate = {2023-05-05},
 year = {2023}
}

@inproceedings{moss,
 author = {Weixin Liang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {8327--8335},
 title = {{MOSS:} End-to-End Dialog System Framework with Modular Supervision},
 year = {2020}
}

@article{mozes2023use,
 author = {Mozes Maximilian et al.},
 journal = {ArXiv preprint},
 title = {Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities},
 volume = {abs/2308.12833},
 year = {2023}
}

@inproceedings{mrksic-etal-2015-multi,
 author = {Mrk{\v{s}}i{\'c} Nikola  et al.},
 booktitle = {ACL},
 pages = {794--799},
 title = {Multi-domain Dialog State Tracking using Recurrent Neural Networks},
 year = {2015}
}

@inproceedings{mrksic-etal-2017-neural,
 author = {Mrk{\v{s}}i{\'c} Nikola  et al.},
 booktitle = {ACL},
 pages = {1777--1788},
 title = {Neural Belief Tracker: Data-Driven Dialogue State Tracking},
 year = {2017}
}

@misc{nakano2022webgpt,
 archiveprefix = {arXiv},
 author = {Reiichiro Nakano et al.},
 eprint = {2112.09332},
 primaryclass = {cs.CL},
 title = {WebGPT: Browser-assisted question-answering with human feedback},
 year = {2022}
}

@inproceedings{nehring-etal-2021-combining,
 author = {Nehring Jan  et al.},
 booktitle = {Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)},
 pages = {38--45},
 title = {Combining Open Domain Question Answering with a Task-Oriented Dialog System},
 year = {2021}
}

@inproceedings{nlu_survey,
 author = {Qin Libo et al.},
 booktitle = {IJCAI},
 note = {Survey Track},
 pages = {4577--4584},
 title = {A Survey on Spoken Language Understanding: Recent Advances and New Frontiers},
 year = {2021}
}

@article{npc,
 author = {Park Joon Sung et al.},
 journal = {ArXiv preprint},
 title = {Generative agents: Interactive simulacra of human behavior},
 volume = {abs/2304.03442},
 year = {2023}
}

@misc{opd,
 author = {Jiaxin Wen et al.},
 title = {OPD: A Chinese Open-Domain Dialogue Pre-trained Model},
 year = {2023}
}

@article{pan2023preliminary,
 author = {Pan Wenbo et al.},
 journal = {ArXiv preprint},
 title = {A preliminary evaluation of chatgpt for zero-shot dialogue understanding},
 volume = {abs/2304.04256},
 year = {2023}
}

@inproceedings{pandey-etal-2018-exemplar,
 author = {Pandey Gaurav  et al.},
 booktitle = {ACL},
 pages = {1329--1338},
 title = {Exemplar Encoder-Decoder for Neural Conversation Generation},
 year = {2018}
}

@inproceedings{papineni-etal-2002-bleu,
 author = {Papineni Kishore  et al.},
 booktitle = {ACL},
 pages = {311--318},
 title = {{B}leu: a Method for Automatic Evaluation of Machine Translation},
 year = {2002}
}

@inproceedings{parthasarathi-pineau-2018-extending,
 author = {Parthasarathi Prasanna  et al.},
 booktitle = {EMNLP},
 pages = {690--695},
 title = {Extending Neural Generative Conversational Model using External Knowledge Sources},
 year = {2018}
}

@misc{patil2023gorilla,
 archiveprefix = {arXiv},
 author = {Shishir G. Patil et al.},
 eprint = {2305.15334},
 primaryclass = {cs.CL},
 title = {Gorilla: Large Language Model Connected with Massive APIs},
 year = {2023}
}

@misc{penedo2023refinedweb,
 archiveprefix = {arXiv},
 author = {Guilherme Penedo et al.},
 eprint = {2306.01116},
 primaryclass = {cs.CL},
 title = {The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only},
 year = {2023}
}

@inproceedings{peng-etal-2020-shot,
 author = {Peng Baolin  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {172--182},
 title = {Few-shot Natural Language Generation for Task-Oriented Dialog},
 year = {2020}
}

@article{peng-etal-2021-soloist,
 author = {Peng Baolin  et al.},
 journal = {TACL},
 pages = {807--824},
 title = {Soloist: Building Task Bots at Scale with Transfer Learning and Machine Teaching},
 volume = {9},
 year = {2021}
}

@inproceedings{peng2017composite,
 author = {Peng Baolin  et al.},
 booktitle = {EMNLP},
 pages = {2231--2240},
 title = {Composite Task-Completion Dialogue Policy Learning via Hierarchical Deep Reinforcement Learning},
 year = {2017}
}

@inproceedings{peng2018adversarial,
 author = {Baolin Peng et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {2018 {IEEE} International Conference on Acoustics, Speech and Signal
Processing, {ICASSP} 2018, Calgary, AB, Canada, April 15-20, 2018},
 pages = {6149--6153},
 title = {Adversarial Advantage Actor-Critic Model for Task-Completion Dialogue
Policy Learning},
 year = {2018}
}

@misc{peng2022godel,
 archiveprefix = {arXiv},
 author = {Baolin Peng et al.},
 eprint = {2206.11309},
 primaryclass = {cs.CL},
 title = {GODEL: Large-Scale Pre-Training for Goal-Directed Dialog},
 year = {2022}
}

@article{peng2023instruction,
 author = {Peng Baolin et al.},
 journal = {ArXiv preprint},
 title = {Instruction tuning with gpt-4},
 volume = {abs/2304.03277},
 year = {2023}
}

@article{peng2023yarn,
 author = {Peng Bowen et al.},
 journal = {ArXiv preprint},
 title = {Yarn: Efficient context window extension of large language models},
 volume = {abs/2309.00071},
 year = {2023}
}

@article{pengfeiliu_survey,
 abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.,&nbsp;the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website including constantly updated survey and paperlist.},
 articleno = {195},
 author = {Liu Pengfei et al.},
 issue_date = {September 2023},
 journal = {ACM Comput. Surv.},
 keywords = {prompting, Pre-trained language models},
 numpages = {35},
 title = {Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
 volume = {55},
 year = {2023}
}

@inproceedings{perez-liu-2017-dialog,
 author = {Perez Julien  et al.},
 booktitle = {EACL},
 pages = {305--314},
 title = {Dialog state tracking, a machine reading approach using Memory Network},
 year = {2017}
}

@inproceedings{persona-consistency,
 abstract = {Generating persona consistent dialogue response is important for developing an intelligent conversational agent. Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response. While simple and effective, our analysis shows that this popular practice is seriously affected by order sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4{\%} on GPT2 and 83.2{\%} on BART). To mitigate the order sensitivity problem, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn robust representation under different persona orders and improve the consistency of response generation. Experiments on the Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (GPT2 and BART).},
 author = {Chen Liang  et al.},
 booktitle = {Finding. of ACL},
 pages = {7337--7345},
 title = {Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization},
 year = {2023}
}

@inproceedings{persona_extending,
 author = {Yifan Liu et al.},
 booktitle = {Proceedings of the 31st {ACM} International Conference on Information {\&} Knowledge Management},
 title = {Improving Personality Consistency in Conversation by Persona Extending},
 year = {2022}
}

@inproceedings{personachat,
 author = {Zhang Saizheng  et al.},
 booktitle = {ACL},
 pages = {2204--2213},
 title = {Personalizing Dialogue Agents: {I} have a dog, do you have pets too?},
 year = {2018}
}

@inproceedings{petroni2019language,
 author = {Petroni Fabio  et al.},
 booktitle = {EMNLP},
 pages = {2463--2473},
 title = {Language Models as Knowledge Bases?},
 year = {2019}
}

@inproceedings{plato,
 author = {Bao Siqi  et al.},
 booktitle = {ACL},
 pages = {85--96},
 title = {{PLATO}: Pre-trained Dialogue Generation Model with Discrete Latent Variable},
 year = {2020}
}

@inproceedings{plato-2,
 author = {Bao Siqi  et al.},
 booktitle = {Finding. of ACL},
 pages = {2513--2525},
 title = {{PLATO-2}: Towards Building an Open-Domain Chatbot via Curriculum Learning},
 year = {2021}
}

@inproceedings{press2022train,
 author = {Ofir Press et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ICLR},
 title = {Train Short, Test Long: Attention with Linear Biases Enables Input
Length Extrapolation},
 year = {2022}
}

@inproceedings{psyqa,
 author = {Sun Hao  et al.},
 booktitle = {Finding. of ACL},
 pages = {1489--1503},
 title = {{P}sy{QA}: A {C}hinese Dataset for Generating Long Counseling Text for Mental Health Support},
 year = {2021}
}

@inproceedings{q-tod,
 author = {Tian Xin  et al.},
 booktitle = {EMNLP},
 pages = {7260--7271},
 title = {{Q}-{TOD}: A Query-driven Task-oriented Dialogue System},
 year = {2022}
}

@article{qi2023fine,
 author = {Qi Xiangyu et al.},
 journal = {ArXiv preprint},
 title = {Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!},
 volume = {abs/2310.03693},
 year = {2023}
}

@inproceedings{qin-etal-2019-stack,
 author = {Qin Libo  et al.},
 booktitle = {EMNLP},
 pages = {2078--2087},
 title = {A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding},
 year = {2019}
}

@inproceedings{qin-etal-2020-dynamic,
 author = {Qin Libo  et al.},
 booktitle = {ACL},
 pages = {6344--6354},
 title = {Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog},
 year = {2020}
}

@misc{qin2023chatgpt,
 archiveprefix = {arXiv},
 author = {Chengwei Qin et al.},
 eprint = {2302.06476},
 primaryclass = {cs.CL},
 title = {Is ChatGPT a General-Purpose Natural Language Processing Task Solver?},
 year = {2023}
}

@misc{qiu2023smile,
 archiveprefix = {arXiv},
 author = {Huachuan Qiu et al.},
 eprint = {2305.00450},
 primaryclass = {cs.CL},
 title = {SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support},
 year = {2023}
}

@inproceedings{ranzato2015sequence,
 author = {Marc'Aurelio Ranzato et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ICLR},
 title = {Sequence Level Training with Recurrent Neural Networks},
 year = {2016}
}

@inproceedings{rocketqav2,
 author = {Ren Ruiyang  et al.},
 booktitle = {EMNLP},
 pages = {2825--2835},
 title = {{R}ocket{QA}v2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking},
 year = {2021}
}

@inproceedings{roller-etal-2021-recipes,
 author = {Roller Stephen  et al.},
 booktitle = {EACL},
 pages = {300--325},
 title = {Recipes for Building an Open-Domain Chatbot},
 year = {2021}
}

@inproceedings{sabour2022cem,
 author = {Sahand Sabour et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {11229--11237},
 title = {{CEM:} Commonsense-Aware Empathetic Response Generation},
 year = {2022}
}

@misc{salemi2023lamp,
 archiveprefix = {arXiv},
 author = {Alireza Salemi et al.},
 eprint = {2304.11406},
 primaryclass = {cs.CL},
 title = {LaMP: When Large Language Models Meet Personalization},
 year = {2023}
}

@misc{schick2023toolformer,
 archiveprefix = {arXiv},
 author = {Timo Schick et al.},
 eprint = {2302.04761},
 primaryclass = {cs.CL},
 title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
 year = {2023}
}

@article{schwartz2013personality,
 author = {Schwartz H Andrew et al.},
 journal = {PloS one},
 pages = {e73791},
 title = {Personality, gender, and age in the language of social media: The open-vocabulary approach},
 volume = {8},
 year = {2013}
}

@article{self_consistency,
 author = {Potsawee Manakul et al.},
 journal = {ArXiv preprint},
 title = {SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for
Generative Large Language Models},
 volume = {abs/2303.08896},
 year = {2023}
}

@article{semnani2023wikichat,
 author = {Semnani Sina J et al.},
 journal = {ArXiv preprint},
 title = {WikiChat: A Few-Shot LLM-Based Chatbot Grounded with Wikipedia},
 volume = {abs/2305.14292},
 year = {2023}
}

@inproceedings{shang2015neural,
 author = {Shang Lifeng  et al.},
 booktitle = {ACL},
 pages = {1577--1586},
 title = {Neural Responding Machine for Short-Text Conversation},
 year = {2015}
}

@article{shen2023hugginggpt,
 author = {Shen Yongliang et al.},
 journal = {ArXiv preprint},
 title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},
 volume = {abs/2303.17580},
 year = {2023}
}

@article{shinn2023reflexion,
 author = {Shinn Noah et al.},
 journal = {ArXiv preprint},
 title = {Reflexion: Language agents with verbal reinforcement learning},
 volume = {abs/2303.11366},
 year = {2023}
}

@inproceedings{shuster-etal-2020-image,
 author = {Shuster Kurt  et al.},
 booktitle = {ACL},
 pages = {2414--2429},
 title = {Image-Chat: Engaging Grounded Conversations},
 year = {2020}
}

@inproceedings{shuster-etal-2021-retrieval-augmentation,
 author = {Shuster Kurt  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {3784--3803},
 title = {Retrieval Augmentation Reduces Hallucination in Conversation},
 year = {2021}
}

@inproceedings{shuster-etal-2022-language,
 author = {Shuster Kurt  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {373--393},
 title = {Language Models that Seek for Knowledge: Modular Search {\&} Generation for Dialogue and Prompt Completion},
 year = {2022}
}

@inproceedings{shuster2021retrieval,
 author = {Shuster Kurt  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {3784--3803},
 title = {Retrieval Augmentation Reduces Hallucination in Conversation},
 year = {2021}
}

@misc{shuster2022blenderbot,
 archiveprefix = {arXiv},
 author = {Kurt Shuster et al.},
 eprint = {2208.03188},
 primaryclass = {cs.CL},
 title = {BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage},
 year = {2022}
}

@inproceedings{SimpleToD,
 author = {Ehsan Hosseini{-}Asl et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 title = {A Simple Language Model for Task-Oriented Dialogue},
 year = {2020}
}

@inproceedings{slot-consistent_nlg,
 author = {Li Yangming  et al.},
 booktitle = {ACL},
 pages = {97--106},
 title = {Slot-consistent {NLG} for Task-oriented Dialogue Systems with Iterative Rectification Network},
 year = {2020}
}

@inproceedings{smith-etal-2020-put,
 author = {Smith Eric Michael  et al.},
 booktitle = {ACL},
 pages = {2021--2030},
 title = {Can You Put it All Together: Evaluating Conversational Agents{'} Ability to Blend Skills},
 year = {2020}
}

@inproceedings{snell2017prototypical,
 author = {Jake Snell et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 pages = {4077--4087},
 title = {Prototypical Networks for Few-shot Learning},
 year = {2017}
}

@inproceedings{song-etal-2021-bob,
 author = {Song Haoyu  et al.},
 booktitle = {ACL},
 pages = {167--177},
 title = {{B}o{B}: {BERT} Over {BERT} for Training Persona-based Dialogue Models from Limited Personalized Data},
 year = {2021}
}

@inproceedings{space-2,
 author = {He Wanwei  et al.},
 booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
 pages = {553--569},
 title = {{SPACE}-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding},
 year = {2022}
}

@inproceedings{space-3,
 abstract = {Recently, pre-training methods have shown remarkable success in task-oriented dialog (TOD) systems. However, most existing pre-trained models for TOD focus on either dialog understanding or dialog generation, but not both. In this paper, we propose SPACE, a novel unified pre-trained dialog model learning from large-scale dialog corpora with limited annotations, which can be effectively fine-tuned on a wide range of downstream dialog tasks. Specifically, SPACE consists of four successive components in a single transformer to maintain a task-flow in TOD systems: (i) a dialog encoding module to encode dialog history, (ii) a dialog understanding module to extract semantic vectors from either user queries or system responses, (iii) a dialog policy module to generate a policy vector that contains high-level semantics of the response, and (iv) a dialog generation module to produce appropriate responses. We design a dedicated pre-training objective for each component. Concretely, we pre-train the dialog encoding module with span mask language modeling to learn contextualized dialog information. To capture the structured dialog semantics, we pre-train the dialog understanding module via a novel tree-induced semi-supervised contrastive learning objective with the help of extra dialog annotations. In addition, we pre-train the dialog policy module by minimizing the ℒ2 distance between its output policy vector and the semantic vector of the response for policy optimization. Finally, the dialog generation model is pre-trained by language modeling. Results show that SPACE achieves state-of-the-art performance on eight downstream dialog benchmarks, including intent prediction, dialog state tracking, and end-to-end dialog modeling. We also show that SPACE has a stronger few-shot ability than existing models under the low-resource setting.},
 author = {He Wanwei et al.},
 booktitle = {SIGIR},
 isbn = {9781450387323},
 keywords = {dialog understanding, task-oriented dialog pre-training, policy planning, dialog generation},
 location = {Madrid, Spain},
 numpages = {14},
 pages = {187–200},
 series = {SIGIR '22},
 title = {Unified Dialog Model Pre-Training for Task-Oriented Dialog Understanding and Generation},
 year = {2022}
}

@inproceedings{stent-etal-2004-trainable,
 author = {Stent Amanda  et al.},
 booktitle = {ACL},
 pages = {79--86},
 title = {Trainable Sentence Planning for Complex Information Presentations in Spoken Dialog Systems},
 year = {2004}
}

@inproceedings{su-etal-2015-reward,
 author = {Su Pei-Hao  et al.},
 booktitle = {SIGDIAL},
 pages = {417--421},
 title = {Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems},
 year = {2015}
}

@inproceedings{su-etal-2016-line,
 author = {Su Pei-Hao  et al.},
 booktitle = {ACL},
 pages = {2431--2441},
 title = {On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems},
 year = {2016}
}

@inproceedings{su-etal-2022-multi,
 author = {Su Yixuan  et al.},
 booktitle = {ACL},
 pages = {4661--4676},
 title = {Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System},
 year = {2022}
}

@article{su2015learning,
 author = {Su Pei-Hao et al.},
 journal = {ArXiv preprint},
 title = {Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems},
 volume = {abs/1508.03386},
 year = {2015}
}

@misc{su2022roformer,
 archiveprefix = {arXiv},
 author = {Jianlin Su et al.},
 eprint = {2104.09864},
 primaryclass = {cs.CL},
 title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
 year = {2022}
}

@misc{sumers2023cognitive,
 archiveprefix = {arXiv},
 author = {Theodore R. Sumers et al.},
 eprint = {2309.02427},
 primaryclass = {cs.AI},
 title = {Cognitive Architectures for Language Agents},
 year = {2023}
}

@inproceedings{sun-etal-2021-adding,
 author = {Sun Kai  et al.},
 booktitle = {NAACL-HLT},
 pages = {1570--1583},
 title = {Adding Chit-Chat to Enhance Task-Oriented Dialogues},
 year = {2021}
}

@inproceedings{sun2022safety,
 author = {Sun Hao  et al.},
 booktitle = {Finding. of ACL},
 pages = {3906--3923},
 title = {On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark},
 year = {2022}
}

@misc{sun2023recitationaugmented,
 archiveprefix = {arXiv},
 author = {Zhiqing Sun et al.},
 eprint = {2210.01296},
 primaryclass = {cs.CL},
 title = {Recitation-Augmented Language Models},
 year = {2023}
}

@article{survey_ds,
 abstract = {Dialogue systems have attracted more and more attention. Recent advances on dialogue systems are overwhelmingly contributed by deep learning techniques, which have been employed to enhance a wide range of big data applications such as computer vision, natural language processing, and recommender systems. For dialogue systems, deep learning can leverage a massive amount of data to learn meaningful feature representations and response generation strategies, while requiring a minimum amount of hand-crafting. In this article, we give an overview to these recent advances on dialogue systems from various perspectives and discuss some possible research directions. In particular, we generally divide existing dialogue systems into task-oriented and nontask- oriented models, then detail how deep learning techniques help them with representative algorithms and finally discuss some appealing research directions that can bring the dialogue system research into a new frontier},
 author = {Chen Hongshen et al.},
 issue_date = {December 2017},
 journal = {SIGKDD Explor. Newsl.},
 numpages = {11},
 pages = {25–35},
 title = {A Survey on Dialogue Systems: Recent Advances and New Frontiers},
 volume = {19},
 year = {2017}
}

@article{systematic_survey,
 author = {Jinjie Ni et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 journal = {Artif. Intell. Rev.},
 pages = {3055--3155},
 title = {Recent advances in deep learning based dialogue systems: a systematic
survey},
 volume = {56},
 year = {2023}
}

@inproceedings{table_qa,
 author = {Cheng Zhoujun  et al.},
 booktitle = {ACL},
 pages = {1094--1110},
 title = {{H}i{T}ab: A Hierarchical Table Dataset for Question Answering and Natural Language Generation},
 year = {2022}
}

@inproceedings{takanobu-etal-2020-goal,
 author = {Takanobu Ryuichi  et al.},
 booktitle = {SIGDIAL},
 pages = {297--310},
 title = {Is Your Goal-Oriented Dialog Model Performing Really Well? Empirical Analysis of System-wise Evaluation},
 year = {2020}
}

@inproceedings{takanobu-etal-2020-multi,
 author = {Takanobu Ryuichi  et al.},
 booktitle = {ACL},
 pages = {625--638},
 title = {Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition},
 year = {2020}
}

@misc{tan2016lstmbased,
 archiveprefix = {arXiv},
 author = {Ming Tan et al.},
 eprint = {1511.04108},
 primaryclass = {cs.CL},
 title = {LSTM-based Deep Learning Models for Non-factoid Answer Selection},
 year = {2016}
}

@article{tausczik2010psychological,
 author = {Tausczik Yla R et al.},
 journal = {Journal of language and social psychology},
 pages = {24--54},
 title = {The psychological meaning of words: LIWC and computerized text analysis methods},
 volume = {29},
 year = {2010}
}

@inproceedings{Technology2019,
 author = {Moscow Institute of Physics et al.},
 booktitle = {Alexa Prize SocialBot Grand Challenge 3 Proceedings},
 title = {DREAM technical report for the Alexa Prize 2019},
 year = {2019}
}

@misc{thoppilan2022lamda,
 archiveprefix = {arXiv},
 author = {Romal Thoppilan et al.},
 eprint = {2201.08239},
 primaryclass = {cs.CL},
 title = {LaMDA: Language Models for Dialog Applications},
 year = {2022}
}

@article{thulke2021efficient,
 author = {Thulke David et al.},
 journal = {ArXiv preprint},
 title = {Efficient retrieval augmented generation from unstructured knowledge for task-oriented dialog},
 volume = {abs/2102.04643},
 year = {2021}
}

@inproceedings{toneva2018empirical,
 author = {Mariya Toneva et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ICLR},
 title = {An Empirical Study of Example Forgetting during Deep Neural Network
Learning},
 year = {2019}
}

@misc{touvron2023llama,
 archiveprefix = {arXiv},
 author = {Hugo Touvron et al.},
 eprint = {2302.13971},
 primaryclass = {cs.CL},
 title = {LLaMA: Open and Efficient Foundation Language Models},
 year = {2023}
}

@misc{touvron2023llama2,
 archiveprefix = {arXiv},
 author = {Hugo Touvron et al.},
 eprint = {2307.09288},
 primaryclass = {cs.CL},
 title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
 year = {2023}
}

@inproceedings{tran-etal-2017-neural,
 author = {Tran Van-Khanh  et al.},
 booktitle = {Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue},
 pages = {231--240},
 title = {Neural-based Natural Language Generation in Dialogue using {RNN} Encoder-Decoder with Semantic Aggregation},
 year = {2017}
}

@inproceedings{trivedi-etal-2023-interleaving,
 abstract = {Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \textit{what to retrieve} depends on \textit{what has already been derived}, which in turn may depend on \textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.},
 author = {Trivedi Harsh  et al.},
 booktitle = {Proc. of ACL},
 pages = {10014--10037},
 title = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
 year = {2023}
}

@misc{tu2023characterchat,
 archiveprefix = {arXiv},
 author = {Quan Tu et al.},
 eprint = {2308.10278},
 primaryclass = {cs.CL},
 title = {CharacterChat: Learning towards Conversational AI with Personalized Social Support},
 year = {2023}
}

@inproceedings{turney2002thumbs,
 author = {Turney, Peter},
 booktitle = {ACL},
 pages = {417--424},
 title = {Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews},
 year = {2002}
}

@inproceedings{types_of_dialogs,
 author = {Liu Zeming  et al.},
 booktitle = {ACL},
 pages = {1024--1034},
 title = {Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals},
 year = {2022}
}

@misc{varshney2023stitch,
 archiveprefix = {arXiv},
 author = {Neeraj Varshney et al.},
 eprint = {2307.03987},
 primaryclass = {cs.CL},
 title = {A Stitch in Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation},
 year = {2023}
}

@inproceedings{vaswani2017attention,
 author = {Ashish Vaswani et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 pages = {5998--6008},
 title = {Attention is All you Need},
 year = {2017}
}

@article{vinyals2015neural,
 author = {Vinyals Oriol et al.},
 journal = {ArXiv preprint},
 title = {A neural conversational model},
 volume = {abs/1506.05869},
 year = {2015}
}

@inproceedings{vinyals2017matching,
 author = {Oriol Vinyals et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 pages = {3630--3638},
 title = {Matching Networks for One Shot Learning},
 year = {2016}
}

@inproceedings{vqa,
 author = {Stanislaw Antol et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015,
Santiago, Chile, December 7-13, 2015},
 pages = {2425--2433},
 title = {{VQA:} Visual Question Answering},
 year = {2015}
}

@inproceedings{wang-etal-2020-learning-efficient,
 author = {Wang Huimin  et al.},
 booktitle = {ACL},
 pages = {6355--6365},
 title = {Learning Efficient Dialogue Policy from Demonstrations through Shaping},
 year = {2020}
}

@inproceedings{wang-etal-2021-fast,
 author = {Wang Dingmin  et al.},
 booktitle = {NAACL-HLT},
 pages = {289--295},
 title = {Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition},
 year = {2021}
}

@inproceedings{wang-etal-2022-prior,
 author = {Wang Zezhong  et al.},
 booktitle = {Proceedings of the 5th International Conference on Natural Language and Speech Processing (ICNLSP 2022)},
 pages = {30--39},
 title = {Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning},
 year = {2022}
}

@inproceedings{wang-etal-2023-retrieval,
 abstract = {Dialogue models are often enriched with extensive external knowledge to provide informative responses through a retrieval-augmented pipeline. Nevertheless, retrieval-augmented approaches rely on finely annotated retrieval training data and knowledge-grounded response generation data, making it costly to transfer. To tackle this challenge, this paper proposed a retrieval-free approach, KiDG, by automatically turning knowledge documents into simulated multi-turn dialogues through a Multi-Document Traversal algorithm. The simulated knowledge-intensive dialogues constructed by KiDG in one domain can be easily used to train and enhance pre-trained dialogue models{'} knowledge w.r.t. this domain without costly annotation. We conduct extensive experiments comparing retrieval-augmented models and a variety of retrieval-free models. We found that dialogue models enhanced with data simulated with KiDG largely outperform state-of-the-art retrieval-free methods, and it achieves comparable performance compared to retrieval-augmented methods while being better, and cheaper at domain transfer.},
 author = {Wang Rui  et al.},
 booktitle = {Proc. of ACL},
 pages = {6608--6619},
 title = {Retrieval-free Knowledge Injection through Multi-Document Traversal for Dialogue Models},
 year = {2023}
}

@article{wang2021ernie,
 author = {Wang Shuohuan et al.},
 journal = {ArXiv preprint},
 title = {Ernie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation},
 volume = {abs/2112.12731},
 year = {2021}
}

@misc{wang2021kddres,
 archiveprefix = {arXiv},
 author = {Hongru Wang et al.},
 eprint = {2011.08772},
 primaryclass = {cs.CL},
 title = {KddRES: A Multi-level Knowledge-driven Dialogue Dataset for Restaurant Towards Customized Dialogue System},
 year = {2021}
}

@misc{wang2021mcml,
 archiveprefix = {arXiv},
 author = {Hongru Wang et al.},
 eprint = {2108.11635},
 primaryclass = {cs.AI},
 title = {MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging},
 year = {2021}
}

@misc{wang2021openvidial,
 archiveprefix = {arXiv},
 author = {Shuhe Wang et al.},
 eprint = {2109.12761},
 primaryclass = {cs.CL},
 title = {OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts},
 year = {2021}
}

@inproceedings{wang2021topicrefine,
 author = {Wang Hongru  et al.},
 booktitle = {Proceedings of the 5th International Conference on Natural Language and Speech Processing (ICNLSP 2022)},
 pages = {19--29},
 title = {{T}opic{R}efine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System},
 year = {2022}
}

@article{wang2023emotional,
 author = {Wang Xuena et al.},
 journal = {ArXiv preprint},
 title = {Emotional intelligence of large language models},
 volume = {abs/2307.09042},
 year = {2023}
}

@misc{wang2023interactive,
 archiveprefix = {arXiv},
 author = {Zekun Wang et al.},
 eprint = {2305.13246},
 primaryclass = {cs.CL},
 title = {Interactive Natural Language Processing},
 year = {2023}
}

@misc{wang2023large,
 archiveprefix = {arXiv},
 author = {Hongru Wang et al.},
 eprint = {2310.08840},
 primaryclass = {cs.CL},
 title = {Large Language Models as Source Planner for Personalized Knowledge-grounded Dialogue},
 year = {2023}
}

@misc{wang2023planandsolve,
 archiveprefix = {arXiv},
 author = {Lei Wang et al.},
 eprint = {2305.04091},
 primaryclass = {cs.CL},
 title = {Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models},
 year = {2023}
}

@misc{wang2023recursively,
 archiveprefix = {arXiv},
 author = {Qingyue Wang et al.},
 eprint = {2308.15022},
 primaryclass = {cs.CL},
 title = {Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models},
 year = {2023}
}

@misc{wang2023selfcritique,
 archiveprefix = {arXiv},
 author = {Rui Wang et al.},
 eprint = {2305.13733},
 primaryclass = {cs.CL},
 title = {Self-Critique Prompting with Large Language Models for Inductive Instructions},
 year = {2023}
}

@misc{wang2023selfguard,
 archiveprefix = {arXiv},
 author = {Zezhong Wang et al.},
 eprint = {2310.15851},
 primaryclass = {cs.CL},
 title = {Self-Guard: Empower the LLM to Safeguard Itself},
 year = {2023}
}

@misc{wang2023survey,
 archiveprefix = {arXiv},
 author = {Cunxiang Wang et al.},
 eprint = {2310.07521},
 primaryclass = {cs.CL},
 title = {Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity},
 year = {2023}
}

@article{wang2023tpe,
 author = {Wang Hongru et al.},
 journal = {ArXiv preprint},
 title = {TPE: Towards Better Compositional Reasoning over Conceptual Tools with Multi-persona Collaboration},
 volume = {abs/2309.16090},
 year = {2023}
}

@inproceedings{wang_multi-domain_2020,
 author = {Wang Kai  et al.},
 booktitle = {ACL},
 pages = {7125--7134},
 title = {Multi-Domain Dialogue Acts and Response Co-Generation},
 year = {2020}
}

@article{wei2022emergent,
 author = {Wei Jason et al.},
 journal = {ArXiv preprint},
 title = {Emergent abilities of large language models},
 volume = {abs/2206.07682},
 year = {2022}
}

@inproceedings{weibo,
 author = {Shang Lifeng  et al.},
 booktitle = {ACL},
 pages = {1577--1586},
 title = {Neural Responding Machine for Short-Text Conversation},
 year = {2015}
}

@inproceedings{weidinger2022taxonomy,
 author = {Weidinger Laura et al.},
 booktitle = {FAccT},
 pages = {214--229},
 title = {Taxonomy of risks posed by language models},
 year = {2022}
}

@inproceedings{welch-etal-2022-leveraging,
 author = {Welch Charles  et al.},
 booktitle = {ACL},
 pages = {1742--1752},
 title = {Leveraging Similar Users for Personalized Language Modeling with Limited Data},
 year = {2022}
}

@inproceedings{wen-etal-2015-semantically,
 author = {Wen Tsung-Hsien  et al.},
 booktitle = {EMNLP},
 pages = {1711--1721},
 title = {Semantically Conditioned {LSTM}-based Natural Language Generation for Spoken Dialogue Systems},
 year = {2015}
}

@inproceedings{wen-etal-2017-network,
 author = {Wen Tsung-Hsien  et al.},
 booktitle = {EACL},
 pages = {438--449},
 title = {A Network-based End-to-End Trainable Task-oriented Dialogue System},
 year = {2017}
}

@inproceedings{weston-etal-2018-retrieve,
 author = {Weston Jason  et al.},
 booktitle = {Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI}},
 pages = {87--92},
 title = {Retrieve and Refine: Improved Sequence Generation Models For Dialogue},
 year = {2018}
}

@inproceedings{whang2021response,
 author = {Taesun Whang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {14041--14049},
 title = {Do Response Selection Models Really Know What's Next? Utterance Manipulation
Strategies for Multi-turn Response Selection},
 year = {2021}
}

@misc{workshop2023bloom,
 archiveprefix = {arXiv},
 author = {BigScience Workshop et al.},
 eprint = {2211.05100},
 primaryclass = {cs.CL},
 title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
 year = {2023}
}

@inproceedings{wow,
 author = {Emily Dinan et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {ICLR},
 title = {Wizard of Wikipedia: Knowledge-Powered Conversational Agents},
 year = {2019}
}

@inproceedings{wu-etal-2017-sequential,
 author = {Wu Yu  et al.},
 booktitle = {ACL},
 pages = {496--505},
 title = {Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots},
 year = {2017}
}

@inproceedings{wu-etal-2019-transferable,
 author = {Wu Chien-Sheng  et al.},
 booktitle = {ACL},
 pages = {808--819},
 title = {Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems},
 year = {2019}
}

@inproceedings{wu-etal-2022-ksam,
 author = {Wu Sixing  et al.},
 booktitle = {Finding. of ACL},
 pages = {353--363},
 title = {{KSAM}: Infusing Multi-Source Knowledge into Dialogue Generation via Knowledge Source Aware Multi-Head Decoding},
 year = {2022}
}

@article{wu2023fine,
 author = {Wu Zeqiu et al.},
 journal = {ArXiv preprint},
 title = {Fine-Grained Human Feedback Gives Better Rewards for Language Model Training},
 volume = {abs/2306.01693},
 year = {2023}
}

@inproceedings{xu-etal-2022-beyond,
 author = {Xu Jing  et al.},
 booktitle = {ACL},
 pages = {5180--5197},
 title = {Beyond Goldfish Memory: Long-Term Open-Domain Conversation},
 year = {2022}
}

@inproceedings{xu2021beyond,
 author = {Xu Jing  et al.},
 booktitle = {ACL},
 pages = {5180--5197},
 title = {Beyond Goldfish Memory: Long-Term Open-Domain Conversation},
 year = {2022}
}

@inproceedings{xu2022cosplay,
 author = {Xu Chen et al.},
 booktitle = {SIGIR},
 pages = {201--211},
 title = {COSPLAY: Concept Set Guided Personalized Dialogue Generation Across Both Party Personas},
 year = {2022}
}

@misc{xu2023searchinthechain,
 archiveprefix = {arXiv},
 author = {Shicheng Xu et al.},
 eprint = {2304.14732},
 primaryclass = {cs.CL},
 title = {Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks},
 year = {2023}
}

@misc{xu2023wizardlm,
 archiveprefix = {arXiv},
 author = {Can Xu et al.},
 eprint = {2304.12244},
 primaryclass = {cs.CL},
 title = {WizardLM: Empowering Large Language Models to Follow Complex Instructions},
 year = {2023}
}

@misc{xue2023improving,
 archiveprefix = {arXiv},
 author = {Boyang Xue et al.},
 eprint = {2310.08372},
 primaryclass = {cs.CL},
 title = {Improving Factual Consistency for Knowledge-Grounded Dialogue Systems via Knowledge Enhancement and Alignment},
 year = {2023}
}

@inproceedings{yang2018unsupervised,
 author = {Zichao Yang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {NeurIPS},
 pages = {7298--7309},
 title = {Unsupervised Text Style Transfer using Language Models as Discriminators},
 year = {2018}
}

@inproceedings{yang2021ubar,
 author = {Yunyi Yang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {14230--14238},
 title = {{UBAR:} Towards Fully End-to-End Task-Oriented Dialog System with
{GPT-2}},
 year = {2021}
}

@article{yang2023baichuan,
 author = {Yang Aiyuan et al.},
 journal = {ArXiv preprint},
 title = {Baichuan 2: Open large-scale language models},
 volume = {abs/2309.10305},
 year = {2023}
}

@inproceedings{yao2013recurrent,
 author = {Yao Kaisheng et al.},
 booktitle = {Interspeech},
 pages = {2524--2528},
 title = {Recurrent neural networks for language understanding.},
 year = {2013}
}

@inproceedings{yao2014spoken,
 author = {Yao Kaisheng et al.},
 booktitle = {SLT},
 pages = {189--194},
 title = {Spoken language understanding using long short-term memory neural networks},
 year = {2014}
}

@misc{yao2023react,
 archiveprefix = {arXiv},
 author = {Shunyu Yao et al.},
 eprint = {2210.03629},
 primaryclass = {cs.CL},
 title = {ReAct: Synergizing Reasoning and Acting in Language Models},
 year = {2023}
}

@misc{yariv2023audiotoken,
 archiveprefix = {arXiv},
 author = {Guy Yariv et al.},
 eprint = {2305.13050},
 primaryclass = {cs.SD},
 title = {AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation},
 year = {2023}
}

@misc{yu2023generate,
 archiveprefix = {arXiv},
 author = {Wenhao Yu et al.},
 eprint = {2209.10063},
 primaryclass = {cs.CL},
 title = {Generate rather than Retrieve: Large Language Models are Strong Context Generators},
 year = {2023}
}

@misc{yu2023promptbased,
 archiveprefix = {arXiv},
 author = {Xiao Yu et al.},
 eprint = {2305.13660},
 primaryclass = {cs.CL},
 title = {Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning},
 year = {2023}
}

@inproceedings{yuan-etal-2019-multi,
 author = {Yuan Chunyuan  et al.},
 booktitle = {EMNLP},
 pages = {111--120},
 title = {Multi-hop Selector Network for Multi-turn Response Selection in Retrieval-based Chatbots},
 year = {2019}
}

@inproceedings{zang-etal-2020-multiwoz,
 author = {Zang Xiaoxue  et al.},
 booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI},
 pages = {109--117},
 title = {{M}ulti{WOZ} 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines},
 year = {2020}
}

@inproceedings{zang-etal-2021-photochat,
 author = {Zang Xiaoxue  et al.},
 booktitle = {ACL},
 pages = {6142--6152},
 title = {{P}hoto{C}hat: A Human-Human Dialogue Dataset With Photo Sharing Behavior For Joint Image-Text Modeling},
 year = {2021}
}

@article{zeng2018linking,
 author = {Zeng Yi et al.},
 journal = {ArXiv preprint},
 title = {Linking artificial intelligence principles},
 volume = {abs/1812.04814},
 year = {2018}
}

@article{zeng2022glm,
 author = {Zeng Aohan et al.},
 journal = {ArXiv preprint},
 title = {Glm-130b: An open bilingual pre-trained model},
 volume = {abs/2210.02414},
 year = {2022}
}

@misc{zeng2023agenttuning,
 archiveprefix = {arXiv},
 author = {Aohan Zeng et al.},
 eprint = {2310.12823},
 primaryclass = {cs.CL},
 title = {AgentTuning: Enabling Generalized Agent Abilities for LLMs},
 year = {2023}
}

@inproceedings{zhang-etal-2018-modeling,
 author = {Zhang Zhuosheng  et al.},
 booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
 pages = {3740--3752},
 title = {Modeling Multi-turn Conversation with Deep Utterance Aggregation},
 year = {2018}
}

@inproceedings{zhang-etal-2020-dialogpt,
 author = {Zhang Yizhe  et al.},
 booktitle = {ACL},
 pages = {270--278},
 title = {{DIALOGPT} : Large-Scale Generative Pre-training for Conversational Response Generation},
 year = {2020}
}

@inproceedings{zhang-etal-2020-find,
 author = {Zhang Jianguo  et al.},
 booktitle = {*SEM},
 pages = {154--167},
 title = {Find or Classify? Dual Strategy for Slot-Value Predictions on Multi-Domain Dialog State Tracking},
 year = {2020}
}

@inproceedings{zhang-etal-2020-mzet,
 author = {Zhang Tao  et al.},
 booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
 pages = {77--87},
 title = {{MZET}: Memory Augmented Zero-Shot Fine-grained Named Entity Typing},
 year = {2020}
}

@inproceedings{zhang2019dialogpt,
 author = {Zhang Yizhe  et al.},
 booktitle = {ACL},
 pages = {270--278},
 title = {{DIALOGPT} : Large-Scale Generative Pre-training for Conversational Response Generation},
 year = {2020}
}

@inproceedings{zhang2020modeling,
 author = {Hainan Zhang et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {IJCAI},
 pages = {3737--3743},
 title = {Modeling Topical Relevance for Multi-Turn Dialogue Generation},
 year = {2020}
}

@misc{zhang2022opt,
 archiveprefix = {arXiv},
 author = {Susan Zhang et al.},
 eprint = {2205.01068},
 primaryclass = {cs.CL},
 title = {OPT: Open Pre-trained Transformer Language Models},
 year = {2022}
}

@misc{zhang2023dialogstudio,
 archiveprefix = {arXiv},
 author = {Jianguo Zhang et al.},
 eprint = {2307.10172},
 primaryclass = {cs.CL},
 title = {DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI},
 year = {2023}
}

@article{zhang2023glm,
 author = {Zhang Jing et al.},
 journal = {ArXiv preprint},
 title = {GLM-Dialog: Noise-tolerant Pre-training for Knowledge-grounded Dialogue Generation},
 volume = {abs/2302.14401},
 year = {2023}
}

@misc{zhang2023memoryaugmented,
 archiveprefix = {arXiv},
 author = {Kai Zhang et al.},
 eprint = {2309.11696},
 primaryclass = {cs.CL},
 title = {Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination},
 year = {2023}
}

@misc{zhang2023sgptod,
 archiveprefix = {arXiv},
 author = {Xiaoying Zhang et al.},
 eprint = {2305.09067},
 primaryclass = {cs.CL},
 title = {SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting},
 year = {2023}
}

@misc{zhang2023videollama,
 archiveprefix = {arXiv},
 author = {Hang Zhang et al.},
 eprint = {2306.02858},
 primaryclass = {cs.CL},
 title = {Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
 year = {2023}
}

@inproceedings{zhao2021unids,
 author = {Zhao Xinyan  et al.},
 booktitle = {Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering},
 pages = {13--22},
 title = {{U}ni{DS}: A Unified Dialogue System for Chit-Chat and Task-oriented Dialogues},
 year = {2022}
}

@article{zhao2023chatgpt,
 author = {Zhao Weixiang et al.},
 journal = {ArXiv preprint},
 title = {Is ChatGPT Equipped with Emotional Dialogue Capabilities?},
 volume = {abs/2304.09582},
 year = {2023}
}

@misc{zhao2023survey,
 archiveprefix = {arXiv},
 author = {Wayne Xin Zhao et al.},
 eprint = {2303.18223},
 primaryclass = {cs.CL},
 title = {A Survey of Large Language Models},
 year = {2023}
}

@article{zhao2023unimc,
 author = {Zhao Kang et al.},
 journal = {ArXiv preprint},
 title = {UniMC: A Unified Framework for Long-Term Memory Conversation via Relevance Representation Learning},
 volume = {abs/2306.10543},
 year = {2023}
}

@inproceedings{zheng-etal-2021-comae,
 author = {Zheng Chujie  et al.},
 booktitle = {Finding. of ACL},
 pages = {813--824},
 title = {{C}o{MAE}: A Multi-factor Hierarchical Framework for Empathetic Response Generation},
 year = {2021}
}

@inproceedings{zheng-etal-2022-mmchat,
 author = {Zheng Yinhe  et al.},
 booktitle = {LREC},
 pages = {5778--5786},
 title = {{MMC}hat: Multi-Modal Chat Dataset on Social Media},
 year = {2022}
}

@article{zheng2019personalized,
 author = {Zheng Yinhe et al.},
 journal = {ArXiv preprint},
 title = {Personalized dialogue generation with diversified traits},
 volume = {abs/1901.09672},
 year = {2019}
}

@misc{zheng2023augesc,
 archiveprefix = {arXiv},
 author = {Chujie Zheng et al.},
 eprint = {2202.13047},
 primaryclass = {cs.CL},
 title = {AugESC: Dialogue Augmentation with Large Language Models for Emotional Support Conversation},
 year = {2023}
}

@article{zheng2023building,
 author = {Zheng Zhonghua et al.},
 journal = {ArXiv preprint},
 title = {Building Emotional Support Chatbots in the Era of LLMs},
 volume = {abs/2308.11584},
 year = {2023}
}

@inproceedings{zhong-etal-2018-global,
 author = {Zhong Victor  et al.},
 booktitle = {ACL},
 pages = {1458--1467},
 title = {Global-Locally Self-Attentive Encoder for Dialogue State Tracking},
 year = {2018}
}

@inproceedings{zhong2021keyword,
 author = {Peixiang Zhong et al.},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 booktitle = {AAAI},
 pages = {14568--14576},
 title = {Keyword-Guided Neural Conversational Model},
 year = {2021}
}

@misc{zhong2023memorybank,
 archiveprefix = {arXiv},
 author = {Wanjun Zhong et al.},
 eprint = {2305.10250},
 primaryclass = {cs.CL},
 title = {MemoryBank: Enhancing Large Language Models with Long-Term Memory},
 year = {2023}
}

@inproceedings{zhou-etal-2016-multi,
 author = {Zhou Xiangyang  et al.},
 booktitle = {EMNLP},
 pages = {372--381},
 title = {Multi-view Response Selection for Human-Computer Conversation},
 year = {2016}
}

@article{zhou-etal-2020-design,
 author = {Zhou Li  et al.},
 journal = {Computational Linguistics},
 pages = {53--93},
 title = {The Design and Implementation of {X}iao{I}ce, an Empathetic Social Chatbot},
 volume = {46},
 year = {2020}
}

@inproceedings{zhou-etal-2020-kdconv,
 author = {Zhou Hao  et al.},
 booktitle = {ACL},
 pages = {7098--7108},
 title = {{K}d{C}onv: A {C}hinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation},
 year = {2020}
}

@inproceedings{zhou-etal-2022-think,
 author = {Zhou Pei  et al.},
 booktitle = {ACL},
 pages = {1237--1252},
 title = {Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation},
 year = {2022}
}

@article{zhou2020design,
 author = {Zhou Li  et al.},
 journal = {Computational Linguistics},
 pages = {53--93},
 title = {The Design and Implementation of {X}iao{I}ce, an Empathetic Social Chatbot},
 volume = {46},
 year = {2020}
}

@article{zhou2021eva,
 author = {Zhou Hao et al.},
 journal = {ArXiv preprint},
 title = {Eva: An open-domain chinese dialogue system with large-scale generative pre-training},
 volume = {abs/2108.01547},
 year = {2021}
}

@article{zhou2022case,
 author = {Zhou Jinfeng et al.},
 journal = {ArXiv preprint},
 title = {CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation},
 volume = {abs/2208.08845},
 year = {2022}
}

@misc{zhou2022link,
 archiveprefix = {arXiv},
 author = {Han Zhou et al.},
 eprint = {2206.14000},
 primaryclass = {cs.CL},
 title = {Link the World: Improving Open-domain Conversation with Dynamic Spatiotemporal-aware Knowledge},
 year = {2022}
}

@inproceedings{zhu-etal-2019-multi,
 author = {Zhu Chenguang  et al.},
 booktitle = {EMNLP},
 pages = {1261--1266},
 title = {Multi-task Learning for Natural Language Generation in Task-Oriented Dialogue},
 year = {2019}
}

@inproceedings{zhu-etal-2020-convlab,
 author = {Zhu Qi  et al.},
 booktitle = {ACL},
 pages = {142--149},
 title = {{C}onv{L}ab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems},
 year = {2020}
}

@inproceedings{zhu-etal-2020-efficient,
 author = {Zhu Su  et al.},
 booktitle = {Finding. of EMNLP},
 pages = {766--781},
 title = {Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking},
 year = {2020}
}

@article{zhu2020crosswoz,
 author = {Zhu Qi  et al.},
 journal = {TACL},
 pages = {281--295},
 title = {{C}ross{WOZ}: A Large-Scale {C}hinese Cross-Domain Task-Oriented Dialogue Dataset},
 volume = {8},
 year = {2020}
}

@article{zhu2020vector,
 author = {Zhu Su et al.},
 journal = {ArXiv preprint},
 title = {Vector Projection Network for Few-shot Slot Tagging in Natural Language Understanding},
 volume = {abs/2009.09568},
 year = {2020}
}
